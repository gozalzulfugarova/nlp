{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2a3898",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1883379586.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    comments = sapply(j$data,function(j){list(comment=j$message)})\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#install.packages(\"RCurl\")\n",
    "library(RCurl)\n",
    "\n",
    "#install.packages(\"rjson\")\n",
    "library(rjson)\n",
    "\n",
    "#install.packages(\"tm\")\n",
    "library(tm)\n",
    "library(Rfacebook)\n",
    "\n",
    "url =\"https://graph.facebook.com/v3.0/2160365850647427/comments?access_token=EAACEdEose0cBAP10gl7Uc246vtZAfwZAXomcwWZC6U8rRkmrSba7MsrZCaEiZCxqMfV4gStaxbZCZACMS1fhJ8UZARwH9nZAPharqiLXXIN24UuV4iipoaGGTG76GLCkHU73QpCRq513VUiP3ivn7dEKmYVXUdeBbSl3vZAiXxLu2qNAeL6oGr1OjPDYLIvmCmnHcZD\"\n",
    "d = getURL(url)\n",
    "d\n",
    "j = fromJSON(d)\n",
    "head(j,2)\n",
    "#install.packages(\"sqldf\")\n",
    "#library(sqldf)\n",
    "comments = sapply(j$data,function(j){list(comment=j$message)})\n",
    "comments\n",
    "comments1 = sapply(j$data,function(j){list(comment=j$message,id =j$id)})\n",
    "comments1\n",
    "#check the str\n",
    "str(comments)\n",
    "getwd()\n",
    "comments.df =as.data.frame(comments1)\n",
    "comments.dfnew= t(comments.df)\n",
    "comments.dfnew[,1]\n",
    "comments.dfnew[,2]\n",
    "head(comments.dfnew,2)\n",
    "colnames(comments.dfnew)\n",
    "#influentialusers<- sqldf(\"select id,comment from comments.dfnew\")\n",
    "#head(influentialusers)\n",
    "write.csv(comments.dfnew, file = \"newFBCommentsWithID.csv\")\n",
    "#comments_new =load(\"newFBComments.csv\")\n",
    "comments_new = read.csv(\"newFBCommentsWithID.csv\", header = TRUE)\n",
    "comments_new\n",
    "\n",
    "\n",
    "\n",
    "#Now cleaning the comments. making a corpus\n",
    "cleanedcomments = sapply(comments, function(x) iconv(enc2utf8(x), sub =\"byte\"))\n",
    "my_corpus = Corpus(VectorSource(cleanedcomments))\n",
    "my_corpus\n",
    "my_function = content_transformer(function(x, pattern)gsub(pattern,\"\",x))\n",
    "my_cleaned_corpus = tm_map(my_corpus,my_function,\"/\")\n",
    "my_cleaned_corpus = tm_map(my_cleaned_corpus,my_function,\"@\")\n",
    "my_cleaned_corpus = tm_map(my_cleaned_corpus,content_transformer(tolower))\n",
    "my_cleaned_corpus = tm_map(my_cleaned_corpus,removeWords, c(stopwords(\"english\"),\"like\",\"so\",\"honey\"))\n",
    "my_cleaned_corpus = tm_map(my_cleaned_corpus,removePunctuation)\n",
    "my_cleaned_corpus = tm_map(my_cleaned_corpus,stripWhitespace)\n",
    "my_cleaned_corpus\n",
    "\n",
    "#cleaned dataframe\n",
    "dataframe <- data.frame(text=sapply(my_cleaned_corpus, identity), \n",
    "                        stringsAsFactors=F)\n",
    "\n",
    "\n",
    "dataframe\n",
    "#count the no. of times a sentence has been used\n",
    "library(plyr)\n",
    "library(dplyr)\n",
    "dataframe %>%count(text, sort = TRUE) \n",
    "write.csv(dataframe, file = \"newcleanedFBComments2.csv\")\n",
    "\n",
    "#second document-stemming\n",
    "#install.packages(\"SnowballC\")\n",
    "#library(SnowballC)\n",
    "stemmingcorpus <- tm_map(my_cleaned_corpus, stemDocument, language = \"english\")\n",
    "stemmingcorpus\n",
    "\n",
    "\n",
    "stemmingdataframe <- data.frame(text=sapply(stemmingcorpus, identity), \n",
    "                                stringsAsFactors=F)\n",
    "stemmingdataframe\n",
    "write.csv(stemmingdataframe, file = \"StemmingFBComments2.csv\")\n",
    "\n",
    "#Remove curse words\n",
    "load_curse_words <- function(curse_words_url) {\n",
    "  connection <- url(curse_words_url)\n",
    "  lines <- readLines(connection)\n",
    "  close(connection)\n",
    "  lines\n",
    "}\n",
    "\n",
    "curse_words <- load_curse_words(\n",
    "  \"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en\"\n",
    ")\n",
    "\n",
    "corpus_without_curse_words <- tm_map(my_cleaned_corpus, removeWords, curse_words)\n",
    "corpus_without_curse_words\n",
    "withoutCursedataframe <- data.frame(text=sapply(corpus_without_curse_words, identity), \n",
    "                                    stringsAsFactors=F)\n",
    "withoutCursedataframe\n",
    "\n",
    "#bigrams&tri-grams\n",
    "#install.packages(\"RWeka\")\n",
    "library(RWeka)\n",
    "library(wordcloud)\n",
    "#bigrams\n",
    "minfreq_bigram=2\n",
    "dataframe\n",
    "\n",
    "as.character(my_cleaned_corpus[[1]])\n",
    "\n",
    "token_delim = \" \\\\t\\\\r\\\\n.!?,;\\\"()\"\n",
    "bitoken = NGramTokenizer(my_cleaned_corpus,Weka_control(min=2,max=2,delimiters = token_delim))\n",
    "bitoken\n",
    "two_word = data.frame(table(bitoken))\n",
    "View(two_word)\n",
    "sort_two = two_word[order(two_word$Freq,decreasing = TRUE),]\n",
    "wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,scale=c(2,0.35),colors = brewer.pal(8,\"Dark2\"),min.freq = minfreq_bigram,max.words=150)\n",
    "\n",
    "\n",
    "# Trigrams \n",
    "\n",
    "minfreq_trigram <-5\n",
    "\n",
    "token_delim <- \" \\\\t\\\\r\\\\n.!?,;\\\"()\"\n",
    "tritoken <- NGramTokenizer(my_cleaned_corpus, Weka_control(min=3,max=3, delimiters = token_delim))\n",
    "three_word <- data.frame(table(tritoken))\n",
    "View(three_word)\n",
    "sort_three <- three_word[order(three_word$Freq,decreasing=TRUE),]\n",
    "wordcloud(sort_three$tritoken, sort_three$Freq, random.order=FALSE,min.freq = minfreq_trigram,scale = c(1.2,0.35),colors = brewer.pal(8,\"Dark2\"),max.words=150)\n",
    "\n",
    "#POS tagging\n",
    "#install.packages(\"openNLP\")\n",
    "#install.packages(\"NLP\")\n",
    "#install.packages(\"rJava\")\n",
    "#install.packages(\"openNLPmodels.en\", repos = \"http://datacube.wu.ac.at/\", type = \"source\")\n",
    "#install.packages(\"stringi\")\n",
    "library(rJava)\n",
    "library(openNLP)\n",
    "library(NLP)\n",
    "sent_token_annotator <- Maxent_Sent_Token_Annotator()\n",
    "word_token_annotator <- Maxent_Word_Token_Annotator()\n",
    "pos_tag_annotator <- Maxent_POS_Tag_Annotator()\n",
    "\n",
    "POS <- NLP::annotate(my_cleaned_corpus, list(sent_token_annotator, word_token_annotator))\n",
    "POS\n",
    "POS_tg <- NLP::annotate(my_cleaned_corpus, pos_tag_annotator, POS)\n",
    "POS_tg\n",
    "\n",
    "POS_tg_subset <- subset(POS_tg, type==\"word\")\n",
    "POS_tg_subset\n",
    "tags <- sapply(POS_tg_subset$features, '[[', \"POS\")\n",
    "# let's see the result\n",
    "head(tags)\n",
    "\n",
    "#Creating a document matrix-to capture everything we take original cleaned corpus\n",
    "my_tdm = TermDocumentMatrix(my_cleaned_corpus)\n",
    "m= as.matrix(my_tdm)\n",
    "View(m)\n",
    "words = sort(rowSums(m),decreasing = TRUE)\n",
    "words\n",
    "#make just one gram words\n",
    "my_data = data.frame(ngram=names(words),freq=words)\n",
    "View(my_data)\n",
    "my_data\n",
    "\n",
    "\n",
    "\n",
    "#rownames(my_data1) <- 1:length(freq)\n",
    "#seeing it visually\n",
    "\n",
    "library(ggplot2)\n",
    "my_data %>%\n",
    "  filter(freq > 1) %>%\n",
    "  mutate(word = reorder(ngram, freq)) %>%\n",
    "  ggplot(aes(word, freq,color=\"blue40\")) +\n",
    "  geom_col() +\n",
    "  xlab(NULL) +\n",
    "  coord_flip()\n",
    "\n",
    "library(RColorBrewer) \n",
    "#install.packages(\"wordcloud\")\n",
    "library(wordcloud)\n",
    "pal2 <- brewer.pal(8,\"Dark2\")\n",
    "wordcloud(words=my_data$ngram,freq = my_data$freq,min.freq=1,max.words=Inf,random.order=FALSE,\n",
    "          scale=c(1.5,0.8),rot.per=0.35, colors=pal2)\n",
    "\n",
    "#POS tagging\n",
    "#devtools::install_github(\"bnosac/RDRPOSTagger\")\n",
    "#devtools::install_github(\"ropensci/tokenizers\")\n",
    "library(\"RDRPOSTagger\")\n",
    "library(\"tokenizers\")\n",
    "\n",
    "#make 2 grams \n",
    "two_ngram_tokenizer <- function(x) \n",
    "  NGramTokenizer(x, Weka_control(min = 2, max = 2))\n",
    "\n",
    "document_term_matrix <- DocumentTermMatrix(my_cleaned_corpus,\n",
    "                                           control = list(tokenize = two_ngram_tokenizer,\n",
    "                                                          wordLengths=c(1, Inf)))\n",
    "\n",
    "two_ngrams <- as.matrix(document_term_matrix)\n",
    "View(two_ngrams)\n",
    "wordsCols = sort(colSums(document_term_matrix),decreasing = TRUE)\n",
    "wordsCols\n",
    "my_data1 = data.frame(ngram=names(wordsCols),freq=wordsCols, stringsAsFactors = FALSE)\n",
    "my_data1\n",
    "my_data1 <- arrange(my_data1, desc(freq))\n",
    "my_data1\n",
    "\n",
    "top_two_ngrams <- two_ngrams[1:50, ]\n",
    "top_two_ngrams\n",
    "\n",
    "\n",
    "#topic modelling\n",
    "#LDA on our document matrix :my_tdm\n",
    "#install.packages(\"topicmodels\")\n",
    "library(topicmodels)\n",
    "#check which documents have zero topics and clean them\n",
    "inspect(my_cleaned_corpus)\n",
    "empty.rows <- dtm[rowTotals == 0, ]$dimnames[1][[1]]\n",
    "my_cleaned_corpus1 <- my_cleaned_corpus[-as.numeric(empty.rows)]\n",
    "my_cleaned_corpus1\n",
    "inspect(my_cleaned_corpus1)\n",
    "#corpus is cleaned\n",
    "\n",
    "my_tdm1 = DocumentTermMatrix(my_cleaned_corpus1)\n",
    "m1= as.matrix(my_tdm1)\n",
    "inspect(my_tdm1)\n",
    "View(m1)\n",
    "#low frequency and high frequency\n",
    "findFreqTerms(my_tdm1, 2,5)\n",
    "\n",
    "#find associations\n",
    "findAssocs(my_tdm1, \"good\", 0.5) \n",
    "findAssocs(my_tdm1, \"animals\", 0.5) \n",
    "findAssocs(my_tdm1, \"life\", 0.5) \n",
    "\n",
    "#controlling the seed and choosing 2 topics\n",
    "data_lda <- LDA(my_tdm1, k = 2, control = list(seed = 1234))\n",
    "data_lda\n",
    "\n",
    "\n",
    "\n",
    "#word-Topic probabilities\n",
    "#install.packages(\"tidytext\")\n",
    "library(tidytext)\n",
    "ap_topics <- tidy(data_lda, matrix = \"beta\")\n",
    "View(ap_topics)\n",
    "\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "ap_top_terms <- ap_topics %>%\n",
    "  group_by(topic) %>%\n",
    "  top_n(10, beta) %>%\n",
    "  ungroup() %>%\n",
    "  arrange(topic, -beta)\n",
    "ap_top_terms\n",
    "\n",
    "ap_top_terms %>%\n",
    "  mutate(term = reorder(term, beta)) %>%\n",
    "  ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ topic, scales = \"free\") +\n",
    "  coord_flip()\n",
    "#results show topic 1 shows the nature/sentiment for the picture from t\n",
    "#topic 2 shows the picture describes the beauty of the animal/creature.\n",
    "#however, picture exists in both topic 1 and topic 2. This will exist in LDA but can be removed by clustering\n",
    "\n",
    "\n",
    "#check for sentiment analysis \n",
    "library(tidytext)\n",
    "my_data_copy=my_data\n",
    "colnames(my_data_copy)[1]=\"word\"\n",
    "colnames(my_data_copy)\n",
    "nrc_joy <- get_sentiments(\"nrc\") %>% \n",
    "  filter(sentiment == \"joy\")\n",
    "nrc_joy\n",
    "#taking out all the joyous words\n",
    "my_data_copy %>%\n",
    "  filter(freq>1) %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE)\n",
    "\n",
    "#most common positive and negative words\n",
    "\n",
    "posNegWords <- my_data_copy %>%\n",
    "  inner_join(get_sentiments(\"bing\")) %>%\n",
    "  count(word, sentiment, sort = TRUE) %>%\n",
    "  ungroup()\n",
    "posNegWords\n",
    "\n",
    "#Visualizing\n",
    "posNegWords %>%\n",
    "  group_by(sentiment) %>%\n",
    "  top_n(15) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(word, n, fill = sentiment)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~sentiment, scales = \"free_y\") +\n",
    "  labs(y = \"Contribution to sentiment\",\n",
    "       x = NULL) +\n",
    "  coord_flip()\n",
    "\n",
    "#see where are the negative words existing--see for problem\n",
    "#install.packages(\"data.table\")\n",
    "library(data.table)\n",
    "commentsDataTable = data.table(comments_new)\n",
    "commentsDataTable\n",
    "#comments_new[comments_new$V1 %like% 'problem',]\n",
    "commentsDataTable[V1 %like% \"problem\"]\n",
    "commentsDataTable[V1 %like% \"reject\"]\n",
    "commentsDataTable[V1 %like% \"hug\"]\n",
    "\n",
    "\n",
    "#Now divide the data into 1-gram and translate the text\n",
    "#use data.table-much faster than sapply and use textcat package\n",
    "one_ngrams_dt <- data.table(my_data)\n",
    "one_ngrams_dt\n",
    "\n",
    "#install.packages(\"textcat\")\n",
    "library(textcat)\n",
    "names(TC_byte_profiles)\n",
    "names(ECIMCI_profiles)\n",
    "#determine the language of our text\n",
    "textcat(dataframe)\n",
    "\n",
    "#can exclude languages if don't want\n",
    "languages <- c(\"english\",\"hindi\")\n",
    "sub_tc_byte_profiles <- TC_byte_profiles[names(TC_byte_profiles) %in% languages]\n",
    "names(sub_tc_byte_profiles)\n",
    "\n",
    "#detect language using textcat\n",
    "language_dt <- one_ngrams_dt[, language := textcat(ngram, p = sub_tc_byte_profiles)]\n",
    "#language_dt <- one_ngrams_dt[, language := textcat(ngram, p = TC_byte_profiles)]\n",
    "language_dt[,]\n",
    "language_dt <- data.table(language_dt)\n",
    "language_aggregated_dt <- language_dt[, sum(freq), by = language]\n",
    "names(language_aggregated_dt) <- c(\"language\", \"frequency\")\n",
    "language_aggregated_dt <- language_aggregated_dt[, percentage := round(frequency / sum(language_dt[, 2]) * 100, 2)]\n",
    "as.data.frame(language_aggregated_dt)\n",
    "language_dt[1:10,]\n",
    "#see which words are taken as hindi\n",
    "language_dt[,which(language_dt$language==\"hindi\")]\n",
    "language_dt[language_dt[,which(language_dt$language==\"hindi\")],]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69becdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
