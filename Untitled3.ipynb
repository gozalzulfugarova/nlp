{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a84843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "print(sent_tokenize(EXAMPLE_TEXT))\n",
    "\t  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed51d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(EXAMPLE_TEXT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaf1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cbc358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d01835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2781d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7ae4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba3f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec36a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d669b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e7566c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b20fa260",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b9dc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5590b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461dbc1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m---> 25\u001b[0m \u001b[43mprocess_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m, in \u001b[0;36mprocess_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         chunkParser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpParser(chunkGram)\n\u001b[0;32m     19\u001b[0m         chunked \u001b[38;5;241m=\u001b[39m chunkParser\u001b[38;5;241m.\u001b[39mparse(tagged)\n\u001b[1;32m---> 20\u001b[0m         \u001b[43mchunked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree\\tree.py:762\u001b[0m, in \u001b[0;36mTree.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03mOpen a new window containing a graphical diagram of this tree.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdraw\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_trees\n\u001b[1;32m--> 762\u001b[0m \u001b[43mdraw_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py:1008\u001b[0m, in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_trees\u001b[39m(\u001b[38;5;241m*\u001b[39mtrees):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    Open a new window containing a graphical diagram of the given\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    trees.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m    :rtype: None\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1008\u001b[0m     \u001b[43mTreeView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py:998\u001b[0m, in \u001b[0;36mTreeView.mainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_idle():\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top\u001b[38;5;241m.\u001b[39mmainloop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f496760b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtree \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunked\u001b[49m\u001b[38;5;241m.\u001b[39msubtrees():\n\u001b[0;32m      2\u001b[0m                \u001b[38;5;28mprint\u001b[39m(subtree)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunked' is not defined"
     ]
    }
   ],
   "source": [
    " for subtree in chunked.subtrees():\n",
    "                print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d30106ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtree \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunked\u001b[49m\u001b[38;5;241m.\u001b[39msubtrees(\u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mlabel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m                \u001b[38;5;28mprint\u001b[39m(subtree)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunked' is not defined"
     ]
    }
   ],
   "source": [
    " for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed03bc77",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m---> 29\u001b[0m \u001b[43mprocess_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m, in \u001b[0;36mprocess_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         chunkParser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpParser(chunkGram)\n\u001b[0;32m     22\u001b[0m         chunked \u001b[38;5;241m=\u001b[39m chunkParser\u001b[38;5;241m.\u001b[39mparse(tagged)\n\u001b[1;32m---> 24\u001b[0m         \u001b[43mchunked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree\\tree.py:762\u001b[0m, in \u001b[0;36mTree.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03mOpen a new window containing a graphical diagram of this tree.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdraw\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_trees\n\u001b[1;32m--> 762\u001b[0m \u001b[43mdraw_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py:1008\u001b[0m, in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_trees\u001b[39m(\u001b[38;5;241m*\u001b[39mtrees):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    Open a new window containing a graphical diagram of the given\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    trees.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m    :rtype: None\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1008\u001b[0m     \u001b[43mTreeView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py:998\u001b[0m, in \u001b[0;36mTreeView.mainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_idle():\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top\u001b[38;5;241m.\u001b[39mmainloop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e32148af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m---> 23\u001b[0m \u001b[43mprocess_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m, in \u001b[0;36mprocess_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         tagged \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(words)\n\u001b[0;32m     17\u001b[0m         namedEnt \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mne_chunk(tagged, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m         \u001b[43mnamedEnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree\\tree.py:762\u001b[0m, in \u001b[0;36mTree.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03mOpen a new window containing a graphical diagram of this tree.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdraw\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_trees\n\u001b[1;32m--> 762\u001b[0m \u001b[43mdraw_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py:1008\u001b[0m, in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_trees\u001b[39m(\u001b[38;5;241m*\u001b[39mtrees):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    Open a new window containing a graphical diagram of the given\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    trees.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m    :rtype: None\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1008\u001b[0m     \u001b[43mTreeView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py:998\u001b[0m, in \u001b[0;36mTreeView.mainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_idle():\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top\u001b[38;5;241m.\u001b[39mmainloop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5666a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce08898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa93d8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Common locations on Windows:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnltk_data\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnltk_data\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnltk_data\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39mprefix, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk_data\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m      6\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39mprefix, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk_data\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m      7\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPPDATA\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk_data\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m     ]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Common locations on UNIX & OS X:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "if sys.platform.startswith('win'):\n",
    "    # Common locations on Windows:\n",
    "    path += [\n",
    "        str(r'C:\\nltk_data'), str(r'D:\\nltk_data'), str(r'E:\\nltk_data'),\n",
    "        os.path.join(sys.prefix, str('nltk_data')),\n",
    "        os.path.join(sys.prefix, str('lib'), str('nltk_data')),\n",
    "        os.path.join(os.environ.get(str('APPDATA'), str('C:\\\\')), str('nltk_data'))\n",
    "    ]\n",
    "else:\n",
    "    # Common locations on UNIX & OS X:\n",
    "    path += [\n",
    "        str('/usr/share/nltk_data'),\n",
    "        str('/usr/local/share/nltk_data'),\n",
    "        str('/usr/lib/nltk_data'),\n",
    "        str('/usr/local/lib/nltk_data')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e10365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sample text\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for x in range(5):\n",
    "    print(tok[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65cd0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "964375ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49ed5c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e2410ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6955f327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d04850b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af33e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'serious', 'honorable', 'dependable', 'upright', 'estimable', 'skillful', 'goodness', 'commodity', 'skilful', 'honest', 'in_effect', 'salutary', 'safe', 'just', 'unspoilt', 'dear', 'secure', 'trade_good', 'well', 'right', 'expert', 'respectable', 'sound', 'effective', 'practiced', 'proficient', 'adept', 'soundly', 'in_force', 'near', 'unspoiled', 'ripe', 'beneficial', 'undecomposed', 'full', 'good', 'thoroughly'}\n",
      "{'ill', 'badness', 'evil', 'bad', 'evilness'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7db9fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e34e05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "774d9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "949af7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['plot', 'summary', ':', 'the', 'year', 'is', '2024', '.', 'the', 'ozone', 'layer', 'has', 'long', 'since', 'gone', 'and', 'the', 'earth', 'is', 'now', 'protected', 'from', 'radiation', 'by', 'a', 'shield', 'invented', 'by', 'connor', 'mccleod', '.', 'the', 'shield', ',', 'although', 'saving', 'lives', ',', 'has', 'made', 'the', 'atmosphere', 'hot', 'and', 'humid', '.', 'connor', 'is', 'a', 'mortal', 'and', 'a', 'tired', 'old', 'man', 'who', 'has', 'given', 'up', 'hope', '.', 'on', 'returning', 'from', 'the', 'opera', 'one', 'night', 'connor', 'is', 'accosted', 'by', 'the', 'leader', 'of', 'a', '\"', 'terrorist', '\"', 'organisation', '.', 'they', 'have', 'tried', 'to', 'find', 'out', 'what', 'the', 'company', 'that', 'now', 'runs', 'the', 'shield', 'unit', 'are', 'covering', 'up', '.', 'connor', 'is', 'then', 'attacked', 'by', 'two', 'assassins', 'sent', 'by', 'katana', 'from', 'the', 'planet', 'zeist', '(', 'like', 'zeit', '--', 'get', 'it', '?', ')', '.', 'they', 'fail', 'and', 'connor', 'becomes', 'immortal', 'again', 'by', 'chopping', 'their', 'heads', 'off', '.', 'he', 'then', 'brings', 'ramirez', 'back', 'to', 'life', 'in', 'glencoe', 'in', 'scotland', '.', 'both', 'ramirez', 'and', 'connor', 'are', 'rebel', 'leaders', 'back', 'on', 'zeist', 'and', 'have', 'been', 'sent', 'to', 'earth', 'as', 'punishment', '.', 'connor', 'now', 'has', 'the', 'option', 'to', 'return', 'being', 'the', 'last', 'one', 'but', 'opts', 'to', 'stay', 'with', 'his', 'new', 'found', 'immortality', 'and', 'fight', 'to', 'find', 'out', 'about', 'the', 'company', 'and', 'the', 'shield', '.', 'katana', 'meanwhile', ',', 'fearing', 'connor', 'would', 'return', ',', 'sets', 'out', 'himself', 'to', 'kill', 'the', 'highlander', '.', 'if', 'you', 'have', 'never', 'seen', 'it', '.', 'being', 'a', 'fan', 'of', 'highlander', 'and', 'mr', '.', 'connery', ',', 'i', 'was', 'in', 'the', 'cinema', 'as', 'soon', 'as', 'i', 'could', 'to', 'see', 'highlander', 'ii', ':', 'the', 'quickening', '.', 'i', 'wish', 'it', 'had', 'been', 'quicker', '.', 'i', 'feel', 'whatever', 'else', 'you', 'can', 'say', 'about', 'a', 'film', '(', 'bad', 'script', 'bad', 'music', ',', 'etc', '.', ')', 'if', 'you', 'sit', 'just', 'plain', 'bored', 'through', 'most', 'of', 'it', 'then', 'nothing', 'much', 'else', 'needs', 'to', 'be', 'said', '.', 'the', 'film', 'was', 'short', 'but', 'in', 'my', 'opinion', 'only', 'twelve', 'minutes', 'or', 'so', '(', 'the', 'time', 'mr', '.', 'connery', 'is', 'on', 'the', 'screen', 'for', ')', 'is', 'worth', 'watching', '.', 'the', 'plot', 'sounds', 'a', 'little', 'ridiculous', 'but', 'i', 'was', 'willing', 'to', 'give', 'it', 'a', 'chance', '.', 'maybe', 'they', 'could', 'pull', 'it', 'off', '.', 'instead', ',', 'i', 'find', 'that', 'the', 'characters', 'have', 'completely', 'changed', 'from', 'those', 'in', 'the', 'first', 'film', '.', 'it', \"'\", 's', 'as', 'if', 'they', 'are', 'different', 'people', 'with', 'the', 'same', 'names', '.', 'witness', 'connor', 'jumping', 'into', 'bed', 'with', 'the', 'leader', 'of', 'the', 'resistance', '--', 'where', 'is', 'the', 'attitude', 'of', '\"', 'who', 'wants', 'to', 'live', 'forever', ',', 'when', 'love', 'must', 'die', '\"', 'gone', 'to', '?', 'the', 'love', 'sub', '-', 'plot', '*', 'must', '*', 'have', 'had', 'footage', 'cut', 'out', '.', 'the', 'highlander', 'and', 'ramirez', 'both', 'seemed', 'aware', 'of', 'powers', 'they', 'had', 'never', 'mentioned', 'before', '.', 'putting', 'our', 'heroes', 'in', 'deadly', 'situations', 'and', 'then', 'having', 'them', 'walk', 'away', 'because', 'of', 'some', 'new', 'power', 'is', 'very', 'annoying', '.', 'i', 'still', 'want', 'to', 'know', 'why', 'connor', \"'\", 's', 'coat', 'becomes', 'flame', 'proof', 'when', 'he', 'is', 'immortal', '!', 'we', 'are', 'not', 'given', 'enough', 'time', 'on', 'zeist', 'to', 'believe', 'in', 'it', ',', 'nor', 'are', 'we', 'told', 'why', 'advanced', 'aliens', 'still', 'use', 'swords', 'to', 'kill', '.', 'why', 'not', 'dynamite', 'or', 'laser', 'saws', 'or', 'just', 'plain', 'bullets', 'followed', 'by', 'a', 'quick', 'chop', 'to', 'the', 'neck', '?', 'i', 'know', 'the', 'head', 'needs', 'to', 'come', 'off', 'but', 'you', 'could', 'at', 'least', 'immobilise', 'your', 'opponent', 'with', 'laser', 'rifles', 'first', '.', 'to', 'be', 'fair', 'one', 'assassin', 'tried', 'this', 'but', 'he', 'must', 'be', 'the', 'world', \"'\", 's', 'worst', 'shot', '.', 'meanwhile', 'connor', 'develops', 'luke', '-', 'skywalker', '-', 'like', 'powers', 'of', 'laser', 'beam', 'deflection', '.', 'as', 'for', '\"', 'bad', 'guys', ',', '\"', 'we', 'see', 'a', 'rebellion', 'on', 'zeist', ',', 'crushed', 'easier', 'than', 'a', 'grape', 'and', 'then', 'the', 'rest', 'of', 'the', 'plot', 'depends', 'on', 'katana', 'being', 'obsessed', 'with', 'killing', 'mccleod', 'in', 'case', 'he', 'returns', 'to', 'zeist', '.', 'two', 'assassins', 'are', 'sent', ',', 'presumably', 'they', 'are', 'meant', 'to', 'be', 'good', 'at', 'killing', 'but', 'an', 'aged', 'connor', 'kills', 'them', 'both', 'with', 'no', 'problems', '.', 'no', 'more', 'assassins', 'are', 'sent', '.', 'katana', ',', 'supposedly', 'a', 'very', 'powerful', 'man', 'goes', 'himself', 'to', 'kill', 'connor', '.', 'has', 'he', 'no', 'more', 'assassins', '?', 'has', 'he', 'no', 'better', '?', 'why', 'was', 'the', 'rebellion', 'crushed', 'so', 'easily', 'then', '?', 'michael', 'ironside', 'is', 'totally', 'unconvincing', '(', 'bring', 'back', 'the', 'kurgan', ')', '.', 'he', 'seems', 'like', 'a', 'cartoon', 'character', 'and', 'really', 'isn', \"'\", 't', 'that', 'evil', ',', 'just', 'stupid', 'and', 'violent', '.', 'we', 'see', 'the', 'hollywood', 'trick', 'of', 'the', 'chief', 'nasty', 'threatening', 'a', 'child', 'again', 'and', 'killing', 'lots', 'of', 'innocents', '.', 'yet', 'he', 'still', 'seems', 'like', 'no', 'threat', '--', 'perhaps', 'because', 'he', 'seems', 'too', 'stupid', 'to', 'threaten', 'connor', ',', 'or', 'maybe', 'because', 'his', 'two', 'best', 'assassins', 'wouldn', \"'\", 't', 'have', 'been', 'amiss', 'with', 'larry', ',', 'curly', 'and', 'mo', '.', 'so', 'much', 'for', 'plot', 'and', 'characters', '.', 'the', 'music', 'is', 'atrocious', '.', 'it', 'is', 'at', 'best', 'intrusive', 'and', 'at', 'worst', 'annoying', '.', 'bring', 'back', 'queen', '.', 'there', 'was', 'an', 'audible', 'sigh', '(', 'of', 'relief', '?', ')', 'when', 'connor', 'played', 'a', 'queen', 'track', 'in', 'a', 'juke', 'box', '.', 'i', 'believe', 'this', 'at', 'least', 'is', 'to', 'be', 'corrected', 'for', 'the', 'american', 'release', '.', 'the', 'acting', 'is', 'at', 'best', 'flat', ',', 'except', 'for', 'mr', '.', 'connery', '.', 'lambert', 'is', 'uninteresting', 'and', 'when', 'he', 'delivered', '\"', 'there', 'can', 'be', 'only', 'one', '\"', 'this', 'time', 'around', 'it', 'just', 'made', 'me', 'wish', 'there', 'had', 'only', 'been', 'one', '--', 'highlander', '.', 'michael', 'ironside', 'is', 'unconvincing', ';', 'sorry', ',', 'michael', ',', 'i', 'usually', 'like', 'you', 'and', 'everyone', 'else', 'was', 'incredibly', 'forgetable', '.', 'so', 'why', '2', '/', '10', '?', 'visually', 'the', 'film', 'is', 'interesting', 'at', 'all', 'times', '.', 'it', 'has', 'a', 'blade', 'runner', 'feel', 'to', 'it', 'and', 'some', 'of', 'the', 'special', 'effects', 'are', 'quite', 'stunning', '.', 'add', 'to', 'this', 'an', 'irrepressible', 'sean', 'connery', 'who', 'smiles', 'throughout', 'the', 'whole', 'film', '--', 'or', 'is', 'he', 'smirking', 'to', 'himself', '?', 'his', 'lines', 'and', 'delivery', 'might', 'make', 'the', 'film', 'worthwhile', 'had', 'it', 'not', 'been', 'so', 'obvious', 'that', 'he', 'cannot', 'take', 'the', 'whole', 'thing', 'seriously', 'at', 'all', 'but', 'hey', '--', '*', 'i', \"'\", 'd', '*', 'have', 'done', 'it', 'for', '12', 'million', '.', 'oh', ',', 'and', 'i', 'agree', 'with', 'a', 'previous', 'reviewer', ',', 'she', '*', 'does', '*', 'look', 'like', 'sharon', 'stone', 'but', 'i', 'believe', 'it', 'is', 'her', 'first', 'film', '.', 'incidentally', 'there', 'is', 'a', 'line', 'producer', 'credited', 'and', 'i', 'hear', 'rumours', 'of', 'highlander', 'iii', ':', 'the', 'wizard', '.'], 'neg')\n",
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(documents[1])\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))\n",
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "331f05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adb24418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5174e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dc40715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac52e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ad7ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plot': True, ':': True, 'two': True, 'teen': True, 'couples': True, 'go': True, 'to': True, 'a': True, 'church': True, 'party': True, ',': True, 'drink': True, 'and': True, 'then': True, 'drive': True, '.': True, 'they': True, 'get': True, 'into': True, 'an': True, 'accident': True, 'one': True, 'of': True, 'the': True, 'guys': True, 'dies': True, 'but': True, 'his': True, 'girlfriend': True, 'continues': True, 'see': True, 'him': True, 'in': True, 'her': True, 'life': True, 'has': True, 'nightmares': True, 'what': True, \"'\": True, 's': True, 'deal': True, '?': True, 'watch': True, 'movie': True, '\"': True, 'sorta': True, 'find': True, 'out': True, 'critique': True, 'mind': True, '-': True, 'fuck': True, 'for': True, 'generation': True, 'that': True, 'touches': True, 'on': True, 'very': True, 'cool': True, 'idea': True, 'presents': True, 'it': True, 'bad': True, 'package': True, 'which': True, 'is': True, 'makes': True, 'this': True, 'review': True, 'even': True, 'harder': True, 'write': True, 'since': True, 'i': True, 'generally': True, 'applaud': True, 'films': True, 'attempt': True, 'break': True, 'mold': True, 'mess': True, 'with': True, 'your': True, 'head': True, 'such': True, '(': True, 'lost': True, 'highway': True, '&': True, 'memento': True, ')': True, 'there': True, 'are': True, 'good': True, 'ways': True, 'making': True, 'all': True, 'types': True, 'these': True, 'folks': True, 'just': True, 'didn': True, 't': True, 'snag': True, 'correctly': True, 'seem': True, 'have': True, 'taken': True, 'pretty': True, 'neat': True, 'concept': True, 'executed': True, 'terribly': True, 'so': True, 'problems': True, 'well': True, 'its': True, 'main': True, 'problem': True, 'simply': True, 'too': True, 'jumbled': True, 'starts': True, 'off': True, 'normal': True, 'downshifts': True, 'fantasy': True, 'world': True, 'you': True, 'as': True, 'audience': True, 'member': True, 'no': True, 'going': True, 'dreams': True, 'characters': True, 'coming': True, 'back': True, 'from': True, 'dead': True, 'others': True, 'who': True, 'look': True, 'like': True, 'strange': True, 'apparitions': True, 'disappearances': True, 'looooot': True, 'chase': True, 'scenes': True, 'tons': True, 'weird': True, 'things': True, 'happen': True, 'most': True, 'not': True, 'explained': True, 'now': True, 'personally': True, 'don': True, 'trying': True, 'unravel': True, 'film': True, 'every': True, 'when': True, 'does': True, 'give': True, 'me': True, 'same': True, 'clue': True, 'over': True, 'again': True, 'kind': True, 'fed': True, 'up': True, 'after': True, 'while': True, 'biggest': True, 'obviously': True, 'got': True, 'big': True, 'secret': True, 'hide': True, 'seems': True, 'want': True, 'completely': True, 'until': True, 'final': True, 'five': True, 'minutes': True, 'do': True, 'make': True, 'entertaining': True, 'thrilling': True, 'or': True, 'engaging': True, 'meantime': True, 'really': True, 'sad': True, 'part': True, 'arrow': True, 'both': True, 'dig': True, 'flicks': True, 'we': True, 'actually': True, 'figured': True, 'by': True, 'half': True, 'way': True, 'point': True, 'strangeness': True, 'did': True, 'start': True, 'little': True, 'bit': True, 'sense': True, 'still': True, 'more': True, 'guess': True, 'bottom': True, 'line': True, 'movies': True, 'should': True, 'always': True, 'sure': True, 'before': True, 'given': True, 'password': True, 'enter': True, 'understanding': True, 'mean': True, 'showing': True, 'melissa': True, 'sagemiller': True, 'running': True, 'away': True, 'visions': True, 'about': True, '20': True, 'throughout': True, 'plain': True, 'lazy': True, '!': True, 'okay': True, 'people': True, 'chasing': True, 'know': True, 'need': True, 'how': True, 'giving': True, 'us': True, 'different': True, 'offering': True, 'further': True, 'insight': True, 'down': True, 'apparently': True, 'studio': True, 'took': True, 'director': True, 'chopped': True, 'themselves': True, 'shows': True, 'might': True, 've': True, 'been': True, 'decent': True, 'here': True, 'somewhere': True, 'suits': True, 'decided': True, 'turning': True, 'music': True, 'video': True, 'edge': True, 'would': True, 'actors': True, 'although': True, 'wes': True, 'bentley': True, 'seemed': True, 'be': True, 'playing': True, 'exact': True, 'character': True, 'he': True, 'american': True, 'beauty': True, 'only': True, 'new': True, 'neighborhood': True, 'my': True, 'kudos': True, 'holds': True, 'own': True, 'entire': True, 'feeling': True, 'unraveling': True, 'overall': True, 'doesn': True, 'stick': True, 'because': True, 'entertain': True, 'confusing': True, 'rarely': True, 'excites': True, 'feels': True, 'redundant': True, 'runtime': True, 'despite': True, 'ending': True, 'explanation': True, 'craziness': True, 'came': True, 'oh': True, 'horror': True, 'slasher': True, 'flick': True, 'packaged': True, 'someone': True, 'assuming': True, 'genre': True, 'hot': True, 'kids': True, 'also': True, 'wrapped': True, 'production': True, 'years': True, 'ago': True, 'sitting': True, 'shelves': True, 'ever': True, 'whatever': True, 'skip': True, 'where': True, 'joblo': True, 'nightmare': True, 'elm': True, 'street': True, '3': True, '7': True, '/': True, '10': True, 'blair': True, 'witch': True, '2': True, 'crow': True, '9': True, 'salvation': True, '4': True, 'stir': True, 'echoes': True, '8': True, 'happy': False, 'bastard': False, 'quick': False, 'damn': False, 'y2k': False, 'bug': False, 'starring': False, 'jamie': False, 'lee': False, 'curtis': False, 'another': False, 'baldwin': False, 'brother': False, 'william': False, 'time': False, 'story': False, 'regarding': False, 'crew': False, 'tugboat': False, 'comes': False, 'across': False, 'deserted': False, 'russian': False, 'tech': False, 'ship': False, 'kick': False, 'power': False, 'within': False, 'gore': False, 'bringing': False, 'few': False, 'action': False, 'sequences': False, 'virus': False, 'empty': False, 'flash': False, 'substance': False, 'why': False, 'was': False, 'middle': False, 'nowhere': False, 'origin': False, 'pink': False, 'flashy': False, 'thing': False, 'hit': False, 'mir': False, 'course': False, 'donald': False, 'sutherland': False, 'stumbling': False, 'around': False, 'drunkenly': False, 'hey': False, 'let': False, 'some': False, 'robots': False, 'acting': False, 'below': False, 'average': False, 'likes': False, 're': False, 'likely': False, 'work': False, 'halloween': False, 'h20': False, 'wasted': False, 'real': False, 'star': False, 'stan': False, 'winston': False, 'robot': False, 'design': False, 'schnazzy': False, 'cgi': False, 'occasional': False, 'shot': False, 'picking': False, 'brain': False, 'if': False, 'body': False, 'parts': False, 'turn': False, 'otherwise': False, 'much': False, 'sunken': False, 'jaded': False, 'viewer': False, 'thankful': False, 'invention': False, 'timex': False, 'indiglo': False, 'based': False, 'late': False, '1960': False, 'television': False, 'show': False, 'name': False, 'mod': False, 'squad': False, 'tells': False, 'tale': False, 'three': False, 'reformed': False, 'criminals': False, 'under': False, 'employ': False, 'police': False, 'undercover': False, 'however': False, 'wrong': False, 'evidence': False, 'gets': False, 'stolen': False, 'immediately': False, 'suspicion': False, 'ads': False, 'cuts': False, 'claire': False, 'dane': False, 'nice': False, 'hair': False, 'cute': False, 'outfits': False, 'car': False, 'chases': False, 'stuff': False, 'blowing': False, 'sounds': False, 'first': False, 'fifteen': False, 'quickly': False, 'becomes': False, 'apparent': False, 'certainly': False, 'slick': False, 'looking': False, 'complete': False, 'costumes': False, 'isn': False, 'enough': False, 'best': False, 'described': False, 'cross': False, 'between': False, 'hour': False, 'long': False, 'cop': False, 'stretched': False, 'span': False, 'single': False, 'clich': False, 'matter': False, 'elements': False, 'recycled': False, 'everything': False, 'already': False, 'seen': False, 'nothing': False, 'spectacular': False, 'sometimes': False, 'bordering': False, 'wooden': False, 'danes': False, 'omar': False, 'epps': False, 'deliver': False, 'their': False, 'lines': False, 'bored': False, 'transfers': False, 'onto': False, 'escape': False, 'relatively': False, 'unscathed': False, 'giovanni': False, 'ribisi': False, 'plays': False, 'resident': False, 'crazy': False, 'man': False, 'ultimately': False, 'being': False, 'worth': False, 'watching': False, 'unfortunately': False, 'save': False, 'convoluted': False, 'apart': False, 'occupying': False, 'screen': False, 'young': False, 'cast': False, 'clothes': False, 'hip': False, 'soundtrack': False, 'appears': False, 'geared': False, 'towards': False, 'teenage': False, 'mindset': False, 'r': False, 'rating': False, 'content': False, 'justify': False, 'juvenile': False, 'older': False, 'information': False, 'literally': False, 'spoon': False, 'hard': False, 'instead': False, 'telling': False, 'dialogue': False, 'poorly': False, 'written': False, 'extremely': False, 'predictable': False, 'progresses': False, 'won': False, 'care': False, 'heroes': False, 'any': False, 'jeopardy': False, 'll': False, 'aren': False, 'basing': False, 'nobody': False, 'remembers': False, 'questionable': False, 'wisdom': False, 'especially': False, 'considers': False, 'target': False, 'fact': False, 'number': False, 'memorable': False, 'can': False, 'counted': False, 'hand': False, 'missing': False, 'finger': False, 'times': False, 'checked': False, 'six': False, 'clear': False, 'indication': False, 'them': False, 'than': False, 'cash': False, 'spending': False, 'dollar': False, 'judging': False, 'rash': False, 'awful': False, 'seeing': False, 'avoid': False, 'at': False, 'costs': False, 'quest': False, 'camelot': False, 'warner': False, 'bros': False, 'feature': False, 'length': False, 'fully': False, 'animated': False, 'steal': False, 'clout': False, 'disney': False, 'cartoon': False, 'empire': False, 'mouse': False, 'reason': False, 'worried': False, 'other': False, 'recent': False, 'challenger': False, 'throne': False, 'last': False, 'fall': False, 'promising': False, 'flawed': False, '20th': False, 'century': False, 'fox': False, 'anastasia': False, 'hercules': False, 'lively': False, 'colorful': False, 'palate': False, 'had': False, 'beat': False, 'hands': False, 'crown': False, '1997': False, 'piece': False, 'animation': False, 'year': False, 'contest': False, 'arrival': False, 'magic': False, 'kingdom': False, 'mediocre': False, '--': False, 'd': False, 'pocahontas': False, 'those': False, 'keeping': False, 'score': False, 'nearly': False, 'dull': False, 'revolves': False, 'adventures': False, 'free': False, 'spirited': False, 'kayley': False, 'voiced': False, 'jessalyn': False, 'gilsig': False, 'early': False, 'daughter': False, 'belated': False, 'knight': False, 'king': False, 'arthur': False, 'round': False, 'table': False, 'dream': False, 'follow': False, 'father': False, 'footsteps': False, 'she': False, 'chance': False, 'evil': False, 'warlord': False, 'ruber': False, 'gary': False, 'oldman': False, 'ex': False, 'gone': False, 'steals': False, 'magical': False, 'sword': False, 'excalibur': False, 'accidentally': False, 'loses': False, 'dangerous': False, 'booby': False, 'trapped': False, 'forest': False, 'help': False, 'hunky': False, 'blind': False, 'timberland': False, 'dweller': False, 'garrett': False, 'carey': False, 'elwes': False, 'headed': False, 'dragon': False, 'eric': False, 'idle': False, 'rickles': False, 'arguing': False, 'itself': False, 'able': False, 'medieval': False, 'sexist': False, 'prove': False, 'fighter': False, 'side': False, 'pure': False, 'showmanship': False, 'essential': False, 'element': False, 'expected': False, 'climb': False, 'high': False, 'ranks': False, 'differentiates': False, 'something': False, 'saturday': False, 'morning': False, 'subpar': False, 'instantly': False, 'forgettable': False, 'songs': False, 'integrated': False, 'computerized': False, 'footage': False, 'compare': False, 'run': False, 'angry': False, 'ogre': False, 'herc': False, 'battle': False, 'hydra': False, 'rest': False, 'case': False, 'stink': False, 'none': False, 'remotely': False, 'interesting': False, 'race': False, 'bland': False, 'end': False, 'tie': False, 'win': False, 'comedy': False, 'shtick': False, 'awfully': False, 'cloying': False, 'least': False, 'signs': False, 'pulse': False, 'fans': False, \"-'\": False, '90s': False, 'tgif': False, 'will': False, 'thrilled': False, 'jaleel': False, 'urkel': False, 'white': False, 'bronson': False, 'balki': False, 'pinchot': False, 'sharing': False, 'nicely': False, 'realized': False, 'though': False, 'm': False, 'loss': False, 'recall': False, 'specific': False, 'providing': False, 'voice': False, 'talent': False, 'enthusiastic': False, 'paired': False, 'singers': False, 'sound': False, 'musical': False, 'moments': False, 'jane': False, 'seymour': False, 'celine': False, 'dion': False, 'must': False, 'strain': False, 'through': False, 'aside': False, 'children': False, 'probably': False, 'adults': False, 'grievous': False, 'error': False, 'lack': False, 'personality': False, 'learn': False, 'goes': False, 'synopsis': False, 'mentally': False, 'unstable': False, 'undergoing': False, 'psychotherapy': False, 'saves': False, 'boy': False, 'potentially': False, 'fatal': False, 'falls': False, 'love': False, 'mother': False, 'fledgling': False, 'restauranteur': False, 'unsuccessfully': False, 'attempting': False, 'gain': False, 'woman': False, 'favor': False, 'takes': False, 'pictures': False, 'kills': False, 'comments': False, 'stalked': False, 'yet': False, 'seemingly': False, 'endless': False, 'string': False, 'spurned': False, 'psychos': False, 'getting': False, 'revenge': False, 'type': False, 'stable': False, 'category': False, '1990s': False, 'industry': False, 'theatrical': False, 'direct': False, 'proliferation': False, 'may': False, 'due': False, 'typically': False, 'inexpensive': False, 'produce': False, 'special': False, 'effects': False, 'stars': False, 'serve': False, 'vehicles': False, 'nudity': False, 'allowing': False, 'frequent': False, 'night': False, 'cable': False, 'wavers': False, 'slightly': False, 'norm': False, 'respect': False, 'psycho': False, 'never': False, 'affair': False, ';': False, 'contrary': False, 'rejected': False, 'rather': False, 'lover': False, 'wife': False, 'husband': False, 'entry': False, 'doomed': False, 'collect': False, 'dust': False, 'viewed': False, 'midnight': False, 'provide': False, 'suspense': False, 'sets': False, 'interspersed': False, 'opening': False, 'credits': False, 'instance': False, 'serious': False, 'sounding': False, 'narrator': False, 'spouts': False, 'statistics': False, 'stalkers': False, 'ponders': False, 'cause': False, 'stalk': False, 'implicitly': False, 'implied': False, 'men': False, 'shown': False, 'snapshot': False, 'actor': False, 'jay': False, 'underwood': False, 'states': False, 'daryl': False, 'gleason': False, 'stalker': False, 'brooke': False, 'daniels': False, 'meant': False, 'called': False, 'guesswork': False, 'required': False, 'proceeds': False, 'begins': False, 'obvious': False, 'sequence': False, 'contrived': False, 'quite': False, 'brings': False, 'victim': False, 'together': False, 'obsesses': False, 'follows': False, 'tries': False, 'woo': False, 'plans': False, 'become': False, 'desperate': False, 'elaborate': False, 'include': False, 'cliche': False, 'murdered': False, 'pet': False, 'require': False, 'found': False, 'exception': False, 'cat': False, 'shower': False, 'events': False, 'lead': False, 'inevitable': False, 'showdown': False, 'survives': False, 'invariably': False, 'conclusion': False, 'turkey': False, 'uniformly': False, 'adequate': False, 'anything': False, 'home': False, 'either': False, 'turns': False, 'toward': False, 'melodrama': False, 'overdoes': False, 'words': False, 'manages': False, 'creepy': False, 'pass': False, 'demands': False, 'maryam': False, 'abo': False, 'close': False, 'played': False, 'bond': False, 'chick': False, 'living': False, 'daylights': False, 'equally': False, 'title': False, 'ditzy': False, 'strong': False, 'independent': False, 'business': False, 'owner': False, 'needs': False, 'proceed': False, 'example': False, 'suspicions': False, 'ensure': False, 'use': False, 'excuse': False, 'decides': False, 'return': False, 'toolbox': False, 'left': False, 'place': False, 'house': False, 'leave': False, 'door': False, 'answers': False, 'opens': False, 'wanders': False, 'returns': False, 'enters': False, 'our': False, 'heroine': False, 'danger': False, 'somehow': False, 'parked': False, 'front': False, 'right': False, 'oblivious': False, 'presence': False, 'inside': False, 'whole': False, 'episode': False, 'places': False, 'incredible': False, 'suspension': False, 'disbelief': False, 'questions': False, 'validity': False, 'intelligence': False, 'receives': False, 'highly': False, 'derivative': False, 'somewhat': False, 'boring': False, 'cannot': False, 'watched': False, 'rated': False, 'mostly': False, 'several': False, 'murder': False, 'brief': False, 'strip': False, 'bar': False, 'offensive': False, 'many': False, 'thrillers': False, 'mood': False, 'stake': False, 'else': False, 'capsule': False, '2176': False, 'planet': False, 'mars': False, 'taking': False, 'custody': False, 'accused': False, 'murderer': False, 'face': False, 'menace': False, 'lot': False, 'fighting': False, 'john': False, 'carpenter': False, 'reprises': False, 'ideas': False, 'previous': False, 'assault': False, 'precinct': False, '13': False, 'homage': False, 'himself': False, '0': False, '+': False, 'believes': False, 'fight': False, 'horrible': False, 'writer': False, 'supposedly': False, 'expert': False, 'mistake': False, 'ghosts': False, 'drawn': False, 'humans': False, 'surprisingly': False, 'low': False, 'powered': False, 'alien': False, 'addition': False, 'anybody': False, 'made': False, 'grounds': False, 'sue': False, 'chock': False, 'full': False, 'pieces': False, 'prince': False, 'darkness': False, 'surprising': False, 'managed': False, 'fit': False, 'admittedly': False, 'novel': False, 'science': False, 'fiction': False, 'experience': False, 'terraformed': False, 'walk': False, 'surface': False, 'without': False, 'breathing': False, 'gear': False, 'budget': False, 'mentioned': False, 'gravity': False, 'increased': False, 'earth': False, 'easier': False, 'society': False, 'changed': False, 'advanced': False, 'culture': False, 'women': False, 'positions': False, 'control': False, 'view': False, 'stagnated': False, 'female': False, 'beyond': False, 'minor': False, 'technological': False, 'advances': False, 'less': False, '175': False, 'expect': False, 'change': False, 'ten': False, 'basic': False, 'common': False, 'except': False, 'yes': False, 'replaced': False, 'tacky': False, 'rundown': False, 'martian': False, 'mining': False, 'colony': False, 'having': False, 'criminal': False, 'napolean': False, 'wilson': False, 'desolation': False, 'williams': False, 'facing': False, 'hoodlums': False, 'automatic': False, 'weapons': False, 'nature': False, 'behave': False, 'manner': False, 'essentially': False, 'human': False, 'savages': False, 'lapse': False, 'imagination': False, 'told': False, 'flashback': False, 'entirely': False, 'filmed': False, 'almost': False, 'tones': False, 'red': False, 'yellow': False, 'black': False, 'powerful': False, 'scene': False, 'train': False, 'rushing': False, 'heavy': False, 'sadly': False, 'buildup': False, 'terror': False, 'creates': False, 'looks': False, 'fugitive': False, 'wannabes': False, 'rock': False, 'band': False, 'kiss': False, 'building': False, 'bunch': False, 'sudden': False, 'jump': False, 'sucker': False, 'thinking': False, 'scary': False, 'happening': False, 'standard': False, 'haunted': False, 'shock': False, 'great': False, 'newer': False, 'unimpressive': False, 'digital': False, 'decapitations': False, 'fights': False, 'short': False, 'stretch': False, 'release': False, 'mission': False, 'panned': False, 'reviewers': False, 'better': False, 'rate': False, 'scale': False, 'following': False, 'showed': False, 'liked': False, 'moderately': False, 'classic': False, 'comment': False, 'twice': False, 'ask': False, 'yourself': False, '8mm': False, 'eight': False, 'millimeter': False, 'wholesome': False, 'surveillance': False, 'sight': False, 'values': False, 'becoming': False, 'enmeshed': False, 'seedy': False, 'sleazy': False, 'underworld': False, 'hardcore': False, 'pornography': False, 'bubbling': False, 'beneath': False, 'town': False, 'americana': False, 'sordid': False, 'sick': False, 'depraved': False, 'necessarily': False, 'stop': False, 'order': False, 'satisfy': False, 'twisted': False, 'desires': False, 'position': False, 'influence': False, 'kinds': False, 'demented': False, 'talking': False, 'snuff': False, 'supposed': False, 'documentaries': False, 'victims': False, 'brutalized': False, 'killed': False, 'camera': False, 'joel': False, 'schumacher': False, 'credit': False, 'batman': False, 'robin': False, 'kill': False, 'forever': False, 'client': False, 'thirds': False, 'unwind': False, 'fairly': False, 'conventional': False, 'persons': False, 'drama': False, 'albeit': False, 'particularly': False, 'unsavory': False, 'core': False, 'threatening': False, 'along': False, 'explodes': False, 'violence': False, 'think': False, 'finally': False, 'tags': False, 'ridiculous': False, 'self': False, 'righteous': False, 'finale': False, 'drags': False, 'unpleasant': False, 'trust': False, 'waste': False, 'hours': False, 'nicolas': False, 'snake': False, 'eyes': False, 'cage': False, 'private': False, 'investigator': False, 'tom': False, 'welles': False, 'hired': False, 'wealthy': False, 'philadelphia': False, 'widow': False, 'determine': False, 'whether': False, 'reel': False, 'safe': False, 'documents': False, 'girl': False, 'assignment': False, 'factly': False, 'puzzle': False, 'neatly': False, 'specialized': False, 'skills': False, 'training': False, 'easy': False, 'cops': False, 'toilet': False, 'tanks': False, 'clues': False, 'deeper': False, 'digs': False, 'investigation': False, 'obsessed': False, 'george': False, 'c': False, 'scott': False, 'paul': False, 'schrader': False, 'occasionally': False, 'flickering': False, 'whirs': False, 'sprockets': False, 'winding': False, 'projector': False, 'reminding': False, 'task': False, 'hints': False, 'toll': False, 'lovely': False, 'catherine': False, 'keener': False, 'frustrated': False, 'cleveland': False, 'ugly': False, 'split': False, 'level': False, 'harrisburg': False, 'pa': False, 'condemn': False, 'condone': False, 'subject': False, 'exploits': False, 'irony': False, 'seven': False, 'scribe': False, 'andrew': False, 'kevin': False, 'walker': False, 'vision': False, 'lane': False, 'limited': False, 'hollywood': False, 'product': False, 'snippets': False, 'covering': False, 'later': False, 'joaquin': False, 'phoenix': False, 'far': False, 'adult': False, 'bookstore': False, 'flunky': False, 'max': False, 'california': False, 'cover': False, 'horrid': False, 'screened': False, 'familiar': False, 'revelation': False, 'sexual': False, 'deviants': False, 'indeed': False, 'monsters': False, 'everyday': False, 'neither': False, 'super': False, 'nor': False, 'shocking': False, 'banality': False, 'exactly': False, 'felt': False, 'weren': False, 'nine': False, 'laughs': False, 'months': False, 'terrible': False, 'mr': False, 'hugh': False, 'grant': False, 'huge': False, 'dork': False, 'oral': False, 'sex': False, 'prostitution': False, 'referring': False, 'bugs': False, 'annoying': False, 'adam': False, 'sandler': False, 'jim': False, 'carrey': False, 'eye': False, 'flutters': False, 'nervous': False, 'smiles': False, 'slapstick': False, 'fistfight': False, 'delivery': False, 'room': False, 'culminating': False, 'joan': False, 'cusack': False, 'lap': False, 'paid': False, '$': False, '60': False, 'included': False, 'obscene': False, 'double': False, 'entendres': False, 'obstetrician': False, 'pregnant': False, 'pussy': False, 'size': False, 'hairs': False, 'coat': False, 'nonetheless': False, 'exchange': False, 'cookie': False, 'cutter': False, 'originality': False, 'humor': False, 'successful': False, 'child': False, 'psychiatrist': False, 'psychologist': False, 'scriptwriters': False, 'could': False, 'inject': False, 'unfunny': False, 'kid': False, 'dad': False, 'asshole': False, 'eyelashes': False, 'offers': False, 'smile': False, 'responds': False, 'english': False, 'accent': False, 'attitude': False, 'possibly': False, '_huge_': False, 'beside': False, 'includes': False, 'needlessly': False, 'stupid': False, 'jokes': False, 'olds': False, 'everyone': False, 'shakes': False, 'anyway': False, 'finds': False, 'usual': False, 'reaction': False, 'fluttered': False, 'paves': False, 'possible': False, 'pregnancy': False, 'birth': False, 'gag': False, 'book': False, 'friend': False, 'arnold': False, 'provides': False, 'cacophonous': False, 'funny': False, 'beats': False, 'costumed': False, 'arnie': False, 'dinosaur': False, 'draw': False, 'parallels': False, 'toy': False, 'store': False, 'jeff': False, 'goldblum': False, 'hid': False, 'dreadful': False, 'hideaway': False, 'artist': False, 'fear': False, 'simultaneous': False, 'longing': False, 'commitment': False, 'doctor': False, 'recently': False, 'switch': False, 'veterinary': False, 'medicine': False, 'obstetrics': False, 'joke': False, 'old': False, 'foreign': False, 'guy': False, 'mispronounces': False, 'stereotype': False, 'say': False, 'yakov': False, 'smirnov': False, 'favorite': False, 'vodka': False, 'hence': False, 'take': False, 'volvo': False, 'nasty': False, 'unamusing': False, 'heads': False, 'simultaneously': False, 'groan': False, 'failure': False, 'loud': False, 'failed': False, 'uninspired': False, 'lunacy': False, 'sunset': False, 'boulevard': False, 'arrest': False, 'please': False, 'caught': False, 'pants': False, 'bring': False, 'theaters': False, 'faces': False, '90': False, 'forced': False, 'unauthentic': False, 'anyone': False, 'q': False, '80': False, 'sorry': False, 'money': False, 'unfulfilled': False, 'desire': False, 'spend': False, 'bucks': False, 'call': False, 'road': False, 'trip': False, 'walking': False, 'wounded': False, 'stellan': False, 'skarsg': False, 'rd': False, 'convincingly': False, 'zombified': False, 'drunken': False, 'loser': False, 'difficult': False, 'smelly': False, 'boozed': False, 'reliable': False, 'swedish': False, 'adds': False, 'depth': False, 'significance': False, 'plodding': False, 'aberdeen': False, 'sentimental': False, 'painfully': False, 'mundane': False, 'european': False, 'playwright': False, 'august': False, 'strindberg': False, 'built': False, 'career': False, 'families': False, 'relationships': False, 'paralyzed': False, 'secrets': False, 'unable': False, 'express': False, 'longings': False, 'accurate': False, 'reflection': False, 'strives': False, 'focusing': False, 'pairing': False, 'alcoholic': False, 'tomas': False, 'alienated': False, 'openly': False, 'hostile': False, 'yuppie': False, 'kaisa': False, 'lena': False, 'headey': False, 'gossip': False, 'haven': False, 'spoken': False, 'wouldn': False, 'norway': False, 'scotland': False, 'automobile': False, 'charlotte': False, 'rampling': False, 'sand': False, 'rotting': False, 'hospital': False, 'bed': False, 'cancer': False, 'soap': False, 'opera': False, 'twist': False, 'days': False, 'live': False, 'blitzed': False, 'step': False, 'foot': False, 'plane': False, 'hits': False, 'open': False, 'loathing': False, 'each': False, 'periodic': False, 'stops': False, 'puke': False, 'dashboard': False, 'whenever': False, 'muttering': False, 'rotten': False, 'turned': False, 'sloshed': False, 'viewpoint': False, 'recognizes': False, 'apple': False, 'hasn': False, 'fallen': False, 'tree': False, 'nosebleeds': False, 'snorting': False, 'coke': False, 'sabotages': False, 'personal': False, 'indifference': False, 'restrain': False, 'vindictive': False, 'temper': False, 'ain': False, 'pair': False, 'true': False, 'notes': False, 'unspoken': False, 'familial': False, 'empathy': False, 'note': False, 'repetitively': False, 'bitchy': False, 'screenwriters': False, 'kristin': False, 'amundsen': False, 'hans': False, 'petter': False, 'moland': False, 'fabricate': False, 'series': False, 'contrivances': False, 'propel': False, 'forward': False, 'roving': False, 'hooligans': False, 'drunks': False, 'nosy': False, 'flat': False, 'tires': False, 'figure': False, 'schematic': False, 'convenient': False, 'narrative': False, 'reach': False, 'unveil': False, 'dark': False, 'past': False, 'simplistic': False, 'devices': False, 'trivialize': False, 'conflict': False, 'mainstays': False, 'wannabe': False, 'exists': False, 'purely': False, 'sake': False, 'weak': False, 'unimaginative': False, 'casting': False, 'thwarts': False, 'pivotal': False, 'role': False, 'were': False, 'stronger': False, 'actress': False, 'perhaps': False, 'coast': False, 'performances': False, 'moody': False, 'haunting': False, 'cinematography': False, 'rendering': False, 'pastoral': False, 'ghost': False, 'reference': False, 'certain': False, 'superior': False, 'indie': False, 'intentional': False, 'busy': False, 'using': False, 'furrowed': False, 'brow': False, 'convey': False, 'twitch': False, 'insouciance': False, 'paying': False, 'attention': False, 'maybe': False, 'doing': False, 'reveal': False, 'worthwhile': False, 'earlier': False, 'released': False, '2001': False, 'jonathan': False, 'nossiter': False, 'captivating': False, 'wonders': False, 'disturbed': False, 'parental': False, 'figures': False, 'bound': False, 'ceremonial': False, 'wedlock': False, 'differences': False, 'presented': False, 'significant': False, 'luminous': False, 'diva': False, 'preening': False, 'static': False, 'solid': False, 'performance': False, 'pathetic': False, 'drunk': False, 'emote': False, 'besides': False, 'catatonic': False, 'sorrow': False, 'genuine': False, 'ferocity': False, 'sexually': False, 'charged': False, 'frisson': False, 'during': False, 'understated': False, 'confrontations': False, 'suggest': False, 'gray': False, 'zone': False, 'complications': False, 'accompany': False, 'torn': False, 'romance': False, 'stifled': False, 'curiosity': False, 'thoroughly': False, 'explores': False, 'neurotic': False, 'territory': False, 'delving': False, 'americanization': False, 'greece': False, 'mysticism': False, 'illusion': False, 'deflect': False, 'pain': False, 'overloaded': False, 'willing': False, 'come': False, 'traditional': False, 'ambitious': False, 'sleepwalk': False, 'rhythms': False, 'timing': False, 'driven': False, 'stories': False, 'complexities': False, 'depressing': False, 'answer': False, 'lawrence': False, 'kasdan': False, 'trite': False, 'useful': False, 'grand': False, 'canyon': False, 'steve': False, 'martin': False, 'mogul': False, 'pronounces': False, 'riddles': False, 'answered': False, 'advice': False, 'heart': False, 'french': False, 'sees': False, 'parents': False, 'tim': False, 'roth': False, 'oops': False, 'vows': False, 'taught': False, 'musketeer': False, 'dude': False, 'used': False, 'fourteen': False, 'arrgh': False, 'swish': False, 'zzzzzzz': False, 'original': False, 'lacks': False, 'energy': False, 'next': False, 'hmmmm': False, 'justin': False, 'chambers': False, 'basically': False, 'uncharismatic': False, 'version': False, 'chris': False, 'o': False, 'donnell': False, 'range': False, 'mena': False, 'suvari': False, 'thora': False, 'birch': False, 'dungeons': False, 'dragons': False, 'miscast': False, 'deliveries': False, 'piss': False, 'poor': False, 'ms': False, 'fault': False, 'definitely': False, 'higher': False, 'semi': False, 'saving': False, 'grace': False, 'wise': False, 'irrepressible': False, 'once': False, 'thousand': False, 'god': False, 'beg': False, 'agent': False, 'marketplace': False, 'modern': False, 'day': False, 'roles': False, 'romantic': False, 'gunk': False, 'alright': False, 'yeah': False, 'yikes': False, 'notches': False, 'fellas': False, 'blares': False, 'ear': False, 'accentuate': False, 'annoy': False, 'important': False, 'behind': False, 'recognize': False, 'epic': False, 'fluffy': False, 'rehashed': False, 'cake': False, 'created': False, 'shrewd': False, 'advantage': False, 'kung': False, 'fu': False, 'phenomenon': False, 'test': False, 'dudes': False, 'keep': False, 'reading': False, 'editing': False, 'shoddy': False, 'banal': False, 'stilted': False, 'plentiful': False, 'top': False, 'horse': False, 'carriage': False, 'stand': False, 'opponent': False, 'scampering': False, 'cut': False, 'mouseketeer': False, 'rope': False, 'tower': False, 'jumping': False, 'chords': False, 'hanging': False, 'says': False, '14': False, 'shirt': False, 'strayed': False, 'championing': False, 'fun': False, 'stretches': False, 'atrocious': False, 'lake': False, 'reminded': False, 'school': False, 'cringe': False, 'musketeers': False, 'fat': False, 'raison': False, 'etre': False, 'numbers': False, 'hoping': False, 'packed': False, 'stuntwork': False, 'promoted': False, 'trailer': False, 'major': False, 'swashbuckling': False, 'beginning': False, 'finishes': False, 'juggling': False, 'ladders': False, 'ladder': False, 'definite': False, 'keeper': False, 'regurgitated': False, 'crap': False, 'tell': False, 'deneuve': False, 'placed': False, 'hullo': False, 'barely': False, 'ugh': False, 'small': False, 'annoyed': False, 'trash': False, 'gang': False, 'vow': False, 'stay': False, 'thank': False, 'outlaws': False, '5': False, 'crouching': False, 'tiger': False, 'hidden': False, 'matrix': False, 'replacement': False, 'killers': False, '6': False, 'romeo': False, 'die': False, 'shanghai': False, 'noon': False, 'remembered': False, 'dr': False, 'hannibal': False, 'lecter': False, 'michael': False, 'mann': False, 'forensics': False, 'thriller': False, 'manhunter': False, 'scottish': False, 'brian': False, 'cox': False, 'works': False, 'usually': False, 'schlock': False, 'halfway': False, 'goodnight': False, 'meaty': False, 'substantial': False, 'brilliant': False, 'check': False, 'dogged': False, 'inspector': False, 'opposite': False, 'frances': False, 'mcdormand': False, 'ken': False, 'loach': False, 'agenda': False, 'harrigan': False, 'disturbing': False, 'l': False, 'e': False, '47': False, 'picked': False, 'sundance': False, 'distributors': False, 'scared': False, 'budge': False, 'dares': False, 'speak': False, 'expresses': False, 'seeking': False, 'adolescents': False, 'pad': False, 'bothered': False, 'members': False, 'presentation': False, 'oddly': False, 'empathetic': False, 'light': False, 'tempered': False, 'robust': False, 'listens': False, 'opposed': False, 'friends': False, 'wire': False, 'act': False, 'confused': False, 'lives': False, 'pay': False, 'courtship': False, 'charming': False, 'temptations': False, 'grown': False, 'stands': False, 'island': False, 'expressway': False, 'slices': False, 'malls': False, 'class': False, 'homes': False, 'suburbia': False, 'filmmaker': False, 'cuesta': False, 'uses': False, 'transparent': False, 'metaphor': False, '15': False, 'protagonist': False, 'howie': False, 'franklin': False, 'dano': False, 'reveals': False, 'morbid': False, 'preoccupation': False, 'death': False, 'citing': False, 'deaths': False, 'alan': False, 'j': False, 'pakula': False, 'songwriter': False, 'harry': False, 'chapin': False, 'exit': False, '52': False, 'fascinated': False, 'feelings': False, 'projected': False, 'bright': False, 'move': False, 'force': False, 'complex': False, 'molesters': False, 'beast': False, 'ashamed': False, 'worked': False, 'ill': False, 'advised': False, 'foray': False, 'unnecessary': False, 'padding': False, 'miserable': False, 'bruce': False, 'altman': False, 'seat': False, 'collar': False, 'crime': False, 'degenerate': False, 'youngsters': False, 'kicks': False, 'robbing': False, 'houses': False, 'homoerotic': False, 'shenanigans': False, 'ass': False, 'terrio': False, 'billy': False, 'kay': False, 'handsome': False, 'artful': False, 'dodger': False, 'add': False, 'themes': False, 'suburban': False, 'ennui': False, 'needed': False, 'awkward': False, 'subplots': False, 'concurrently': False, 'relationship': False, 'evenly': False, 'paced': False, 'exceptionally': False, 'acted': False, 'sporting': False, 'baseball': False, 'cap': False, 'faded': False, 'marine': False, 'tattoo': False, 'bluff': False, 'bluster': False, 'quiet': False, 'glance': False, 'withdrawn': False, 'whose': False, 'dramatic': False, 'choices': False, 'broad': False, 'calling': False, 'haley': False, 'restraint': False, 'admirable': False, 'screenplay': False, 'material': False, 'reads': False, 'walt': False, 'whitman': False, 'poem': False, 'moment': False, 'precious': False, 'lingers': False, 'ecstatic': False, 'hearing': False, 'glenn': False, 'gould': False, 'performing': False, 'bach': False, 'goldberg': False, 'variations': False, 'involving': False, 'walter': False, 'masterson': False, 'jealous': False, 'newbie': False, 'thread': False, 'predictably': False, 'leads': False, 'observational': False, 'portrait': False, 'alienation': False, 'royally': False, 'screwed': False, 'terry': False, 'zwigoff': False, 'superb': False, 'confidence': False, 'ambivalent': False, 'typical': False, 'cinema': False, 'wrap': False, 'bullet': False, 'sparing': False, 'writers': False, 'philosophical': False, 'regard': False, 'countless': False, 'share': False, 'blockbuster': False, 'solved': False, 'obstacle': False, 'removed': False, 'often': False, 'extend': False, 'question': False, 'striving': False, 'realism': False, 'destroy': False, 'janeane': False, 'garofalo': False, 'couple': False, 'truth': False, 'cats': False, 'dogs': False, 'excruciating': False, 'matchmaker': False, 'books': False, 'plods': False, 'predestined': False, 'surprises': False, 'jumps': False, 'popular': False, 'political': False, 'satire': False, 'bandwagon': False, 'campaign': False, 'aide': False, 'massacusetts': False, 'senator': False, 'sanders': False, 'reelection': False, 'denis': False, 'leary': False, 'stereotypical': False, 'strategist': False, 'ethics': False, 'scandal': False, 'plagued': False, 'play': False, 'irish': False, 'roots': False, 'boston': False, 'roman': False, 'catholic': False, 'democrat': False, 'contingent': False, 'kennedy': False, 'family': False, 'orders': False, 'ireland': False, 'relatives': False, 'exploit': False, 'soon': False, 'learns': False, 'said': False, 'done': False, 'mantra': False, 'tiny': False, 'misses': False, 'bus': False, 'hotel': False, 'ends': False, 'smallest': False, 'trashiest': False, 'dog': False, 'luggage': False, 'roger': False, 'ebert': False, 'calls': False, 'meet': False, 'happens': False, 'unconventional': False, 'cinematic': False, 'walks': False, 'bathroom': False, 'nude': False, 'sean': False, 'david': False, 'hara': False, 'bathtub': False, 'points': False, 'guessing': False, 'water': False, 'hates': False, 'instant': False, 'saw': False, 'irishman': False, 'hate': False, 'awhile': False, 'succumb': False, 'charms': False, 'happily': False, 'superficial': False, 'detail': False, 'throw': False, 'turmoil': False, 'reconcile': False, 'tune': False, 'annual': False, 'matchmaking': False, 'festival': False, 'lonely': False, 'county': False, 'future': False, 'bliss': False, 'milo': False, 'shea': False, 'snyder': False, 'pops': False, 'onscreen': False, 'spew': False, 'souls': False, 'assured': False, 'match': False, 'utter': False, 'predictability': False, 'message': False, 'respectable': False, 'person': False, 'comedic': False, 'distinction': False, 'sell': False, 'script': False, 'excited': False, 'stays': False, 'stateside': False, 'yelling': False, 'phone': False, 'undoes': False, 'microphone': False, 'speech': False, 'known': False, 'flying': False, 'hong': False, 'kong': False, 'style': False, 'filmmaking': False, 'classics': False, 'nod': False, 'asia': False, 'france': False, 'lukewarm': False, 'dumas': False, 'asian': False, 'stunt': False, 'coordinator': False, 'xing': False, 'xiong': False, 'prior': False, 'attempts': False, 'choreography': False, 'laughable': False, 'van': False, 'damme': False, 'vehicle': False, 'team': False, 'dennis': False, 'rodman': False, 'simon': False, 'sez': False, 'thrown': False, 'air': False, 'result': False, 'tepid': False, 'adventure': False, 'rip': False, 'stinks': False, 'indiana': False, 'jones': False, 'simple': False, 'grandmother': False, 'adapted': False, 'artagnan': False, 'vengeful': False, 'son': False, 'slain': False, 'travels': False, 'paris': False, 'join': False, 'royal': False, 'meets': False, 'cunning': False, 'cardinal': False, 'richelieu': False, 'stephen': False, 'rea': False, 'overthrow': False, 'associate': False, 'febre': False, 'killer': False, 'disbanded': False, 'rounds': False, 'aramis': False, 'nick': False, 'moran': False, 'athos': False, 'jan': False, 'gregor': False, 'kremp': False, 'porthos': False, 'steven': False, 'spiers': False, 'wrongfully': False, 'imprisoned': False, 'leader': False, 'treville': False, 'prison': False, 'frisky': False, 'interest': False, 'chambermaid': False, 'francesca': False, 'footsy': False, 'coo': False, 'hunts': False, 'queen': False, 'captured': False, 'menancing': False, 'forcing': False, 'regroup': False, 'leading': False, 'charge': False, 'peter': False, 'hyams': False, 'wanted': False, 'blend': False, 'eastern': False, 'western': False, 'styles': False, 'disaster': False, 'reality': False, 'ones': False, 'jet': False, 'li': False, 'risk': False, 'ironically': False, 'swordplay': False, 'spread': False, 'carry': False, 'bulk': False, '30': False, 'minute': False, 'picture': False, 'weighs': False, 'monotonous': False, 'gene': False, 'quintano': False, 'prosaic': False, 'wedding': False, 'planner': False, 'mousy': False, 'artangnan': False, 'hyam': False, 'candles': False, 'torches': False, 'grime': False, 'filth': False, '17th': False, 'noted': False, 'standout': False, 'mortal': False, 'kombat': False, 'annihilation': False, 'reviewed': False, 'multiple': False, 'levels': False, 'rampant': False, 'usage': False, 'randian': False, 'subtext': False, 'pervades': False, 'occasionaly': False, 'ironic': False, 'depreciating': False, 'remark': False, 'tosses': False, 'clearly': False, 'marxist': False, 'imagery': False, 'kidding': False, 'seriousness': False, 'fair': False, '*': False, 'necessary': False, 'viewpoints': False, 'watcher': False, 'unfamiliar': False, 'marginally': False, 'fan': False, 'games': False, '1995': False, 'concerned': False, 'martial': False, 'arts': False, 'tournament': False, 'decide': False, 'fate': False, 'billion': False, 'inhabitants': False, 'mortals': False, 'theory': False, 'prevented': False, 'emperor': False, 'shao': False, 'khan': False, 'arriving': False, 'ready': False, 'assumed': False, 'stance': False, 'extraordinarily': False, 'myself': False, 'game': False, 'enjoyed': False, 'directors': False, 'knew': False, 'limitations': False, 'try': False, 'overachieve': False, 'accompanying': False, 'intersperesed': False, 'distracting': False, 'non': False, 'intrusive': False, 'bits': False, 'fluff': False, 'passing': False, 'smashing': False, 'success': False, 'box': False, 'office': False, 'picks': False, 'precisely': False, 'introductory': False, 'exposition': False, 'anyways': False, 'hell': False, 'silly': False, 'rule': False, 'winning': False, 'thereafter': False, 'approximately': False, '85': False, 'alternates': False, 'general': False, 'impression': False, 'producers': False, 'thought': False, 'formula': False, 'volumes': False, 'truly': False, 'sandra': False, 'hess': False, 'sonya': False, 'blade': False, 'execrable': False, 'convince': False, 'loved': False, 'johnny': False, 'greased': False, 'worst': False, 'mis': False, 'james': False, 'remar': False, 'raiden': False, 'thunder': False, 'christopher': False, 'lambert': False, 'japanese': False, 'revered': False, 'chinese': False, 'mystics': False, 'against': False, 'lister': False, 'jr': False, 'president': False, 'u': False, 'fifth': False, 'utility': False, 'totally': False, 'luxury': False, 'amused': False, 'awareness': False, 'introduced': False, 'meaningless': False, 'sidetracks': False, 'including': False, 'muddled': False, 'liu': False, 'kang': False, 'shou': False, 'seeks': False, 'nightwolf': False, 'litefoot': False, 'mystical': False, 'hallucination': False, 'jade': False, 'irina': False, 'pantaeva': False, 'reasons': False, 'unless': False, 'critiques': False, 'apply': False, 'worse': False, 'bridgette': False, 'convincing': False, 'looked': False, 'mimicing': False, 'movements': False, 'choreographer': False, 'knows': False, 'puts': False, 'believable': False, 'jax': False, 'earthquake': False, 'animality': False, 'bonus': False, 'similar': False, 'moves': False, 'mistakenly': False, 'hang': False, 'lamest': False, 'involved': False, 'mud': False, 'wrestling': False, 'lame': False, 'politically': False, 'incorrect': False, 'noticed': False, 'remarked': False, 'upon': False, 'emporer': False, 'perform': False, 'animalities': False, 'motaro': False, 'sheeva': False, 'lifelike': False, 'goro': False, 'enjoy': False, 'femme': False, 'la': False, 'nikita': False, 'backdraft': False, 'sliver': False, 'cindy': False, 'crawford': False, 'anne': False, 'parillaud': False, 'conspire': False, 'shattered': False, 'image': False, 'hooey': False, 'stallone': False, 'stone': False, 'specialist': False, 'poses': False, 'recurring': False, 'assassin': False, 'honeymooning': False, 'jamaica': False, 'believe': False, 'runs': False, 'painful': False, 'pedestrian': False, 'siouxsie': False, 'sioux': False, 'wig': False, 'emotionless': False, 'leather': False, 'clothing': False, 'seattle': False, 'moping': False, 'karen': False, 'endlessly': False, 'complicated': False, 'plots': False, 'helps': False, 'crisp': False, 'modicum': False, 'shakespearean': False, 'begin': False, 'saddled': False, 'leaden': False, 'zero': False, 'breaking': False, 'cardboard': False, 'confines': False, 'insist': False, 'couldn': False, 'huh': False, 'wonderful': False, 'interchange': False, 'charm': False, 'faster': False, 'learned': False, 'cereal': False, 'crybaby': False, 'imagines': False, 'stranger': False, 'sends': False, 'flowers': False, 'chromium': False, 'tough': False, 'nails': False, 'crack': False, 'killing': False, 'machine': False, 'shoots': False, 'mirrors': False, 'stock': False, 'interested': False, 'nest': False, 'egg': False, 'pave': False, 'paradise': False, 'put': False, 'parking': False, 'graham': False, 'greene': False, 'barbet': False, 'schroeder': False, 'reversal': False, 'fortune': False, 'co': False, 'produced': False, 'agonizingly': False, 'b': False, 'york': False, 'vampires': False, 'latest': False, 'opus': False, 'bent': False, 'outing': False, 'aptly': False, 'titled': False, 'suppose': False, 'went': False, 'prefixed': False, 'possessive': False, 'unashamed': False, 'punctuated': False, 'above': False, 'storyline': False, 'borders': False, 'idiotic': False, 'chaotic': False, 'dormant': False, 'martians': False, 'swirling': False, 'gases': False, 'awakened': False, 'meddling': False, 'possess': False, 'hapless': False, 'colonists': False, 'testy': False, 'marilyn': False, 'manson': False, 'lookalikes': False, 'pooh': False, 'bah': False, 'counsel': False, 'official': False, 'melanie': False, 'ballard': False, 'natasha': False, 'henstridge': False, 'sub': False, 'species': False, 'returnee': False, 'officer': False, 'incarcerated': False, 'felon': False, 'second': False, 'blonde': False, 'pulled': False, 'tightly': False, 'awkwardly': False, 'ponytail': False, 'ice': False, 'cube': False, 'appropriately': False, 'named': False, 'pam': False, 'grier': False, 'briefly': False, 'whom': False, 'wonder': False, 'host': False, 'extras': False, 'therefore': False, 'bird': False, 'shots': False, 'sprawling': False, 'metropolis': False, 'reddish': False, 'state': False, 'art': False, 'trademark': False, 'finding': False, 'department': False, 'lock': False, 'laughing': False, 'barrel': False, 'fare': False, 'dingy': False, 'interiors': False, 'cluttered': False, 'exteriors': False, 'inane': False, 'lots': False, 'scarred': False, 'crazed': False, 'aliens': False, 'weaponry': False, 'warfare': False, 'warning': False, 'spontaneously': False, 'stupidly': False, 'villains': False, 'border': False, 'conflicts': False, 'shootouts': False, 'minus': False, 'hissing': False, 'plissken': False, 'miss': False, 'dubbed': False, 'minimalist': False, 'soundtracks': False, 'graduated': False, 'effective': False, 'scoring': False, 'highlighting': False, 'screeching': False, 'guitar': False, 'fortunately': False, 'drowns': False, 'er': False, 'audible': False, 'priceless': False, 'proven': False, 'infertile': False, 'breeding': False, 'ground': False, 'stillborn': False, 'val': False, 'kilmer': False, 'disappointing': False, 'weekend': False, 'overshadowed': False, 'sequels': False, 'among': False, 'pie': False, 'rush': False, 'absent': False, 'references': False, 'set': False, 'perth': False, 'amboys': False, 'closest': False, 'neighbor': False, 'slap': False, 'upside': False, '1': False, 'keeps': False, 'miraculously': False, 'pretend': False, 'means': False, 'intelligent': False, 'grade': False, 'sci': False, 'fi': False, 'singularly': False, 'luck': False, 'starting': False, 'alicia': False, 'silverstone': False, 'beautiful': False, 'creatures': False, 'green': False, 'critic': False, 'large': False, 'choosing': False, 'strikes': False, 'crush': False, 'slow': False, 'moving': False, 'horrific': False, 'adaptation': False, 'clueless': False, 'mailed': False, 'saying': False, 'theater': False, 'expecting': False, 'preview': False, 'crazymadinlove': False, 'whiny': False, 'unlikable': False, 'wasn': False, 'yelled': False, 'f': False, '$&#': False, 'laugh': False, 'agreement': False, 'walked': False, 'babysitter': False, 'inner': False, 'compulsion': False, 'understand': False, 'rent': False, 'regret': False, 'paragraph': False, 'competition': False, 'criticizing': False, 'thin': False, 'shred': False, 'slower': False, 'glacier': False, 'writing': False, 'appeal': False, 'whatsoever': False, 'pointlessly': False, 'concluded': False, 'violent': False, 'plus': False, 'equals': False, 'spends': False, 'twenty': False, 'bubble': False, 'bath': False, 'joined': False, 'four': False, 'features': False, 'settle': False, 'friday': False, 'cocktail': False, 'automatically': False, 'nights': False, 'trods': False, 'discover': False, 'silent': False, 'object': False, 'male': False, 'fantasies': False, 'thinks': False, 'recapture': False, 'youth': False, 'boyfriend': False, 'lets': False, 'wild': False, 'spying': False, 'outside': False, 'prepubescent': False, 'keyhole': False, 'aged': False, 'counterpart': False, 'asked': False, '200': False, 'pound': False, 'silk': False, 'teddy': False, 'fanatasies': False, 'realm': False, 'pg': False, 'imagine': False, 'cinemax': False, 'staple': False, 'absolutely': False, 'pointed': False, 'classify': False, 'mix': False, 'various': False, 'encounters': False, 'third': False, 'space': False, 'odyssey': False, 'apollo': False, 'contact': False, 'hope': False, 'melange': False, 'considering': False, 'disastrous': False, 'results': False, 'sucks': False, 'rescue': False, 'astronauts': False, 'sent': False, '2020': False, 'unknown': False, 'aviators': False, 'visit': False, 'underwhelming': False, 'describe': False, 'uneven': False, 'promise': False, 'buzz': False, 'neutral': False, 'cherry': False, 'colored': False, 'nerdies': False, 'techie': False}\n"
     ]
    }
   ],
   "source": [
    "print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b59f815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23fad4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set that we'll train our classifier with\n",
    "training_set = featuresets[:1900]\n",
    "\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f738a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd170f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 80.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caf6492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =      9.5 : 1.0\n",
      "                 frances = True              pos : neg    =      9.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.3 : 1.0\n",
      "                  annual = True              pos : neg    =      8.6 : 1.0\n",
      "           unimaginative = True              neg : pos    =      8.1 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.4 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.2 : 1.0\n",
      "               atrocious = True              neg : pos    =      6.8 : 1.0\n",
      "                  shoddy = True              neg : pos    =      6.8 : 1.0\n",
      "                obstacle = True              pos : neg    =      6.6 : 1.0\n",
      "                  regard = True              pos : neg    =      6.4 : 1.0\n",
      "                  turkey = True              neg : pos    =      6.3 : 1.0\n",
      "                 kidding = True              neg : pos    =      6.1 : 1.0\n",
      "                    mena = True              neg : pos    =      6.1 : 1.0\n",
      "                  suvari = True              neg : pos    =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d0b8864",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m save_classifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaivebayes.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(classifier, save_classifier)\n\u001b[0;32m      3\u001b[0m save_classifier\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b0321c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaivebayes.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(classifier_f)\n\u001b[0;32m      3\u001b[0m classifier_f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9f55198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ddd5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4923d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy percent: 0.8\n",
      "BernoulliNB accuracy percent: 0.78\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MultinomialNB accuracy percent:\",nltk.classify.accuracy(MNB_classifier, testing_set))\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB accuracy percent:\",nltk.classify.accuracy(BNB_classifier, testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8074c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83ea7813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 80.0\n",
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =      9.5 : 1.0\n",
      "                 frances = True              pos : neg    =      9.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.3 : 1.0\n",
      "                  annual = True              pos : neg    =      8.6 : 1.0\n",
      "           unimaginative = True              neg : pos    =      8.1 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.4 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.2 : 1.0\n",
      "               atrocious = True              neg : pos    =      6.8 : 1.0\n",
      "                  shoddy = True              neg : pos    =      6.8 : 1.0\n",
      "                obstacle = True              pos : neg    =      6.6 : 1.0\n",
      "                  regard = True              pos : neg    =      6.4 : 1.0\n",
      "                  turkey = True              neg : pos    =      6.3 : 1.0\n",
      "                 kidding = True              neg : pos    =      6.1 : 1.0\n",
      "                    mena = True              neg : pos    =      6.1 : 1.0\n",
      "                  suvari = True              neg : pos    =      6.1 : 1.0\n",
      "MNB_classifier accuracy percent: 80.0\n",
      "BernoulliNB_classifier accuracy percent: 78.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 81.0\n",
      "SGDClassifier_classifier accuracy percent: 74.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGDClassifier_classifier accuracy percent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, (nltk\u001b[38;5;241m.\u001b[39mclassify\u001b[38;5;241m.\u001b[39maccuracy(SGDClassifier_classifier, testing_set))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     20\u001b[0m SVC_classifier \u001b[38;5;241m=\u001b[39m SklearnClassifier(SVC())\n\u001b[1;32m---> 21\u001b[0m \u001b[43mSVC_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVC_classifier accuracy percent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, (nltk\u001b[38;5;241m.\u001b[39mclassify\u001b[38;5;241m.\u001b[39maccuracy(SVC_classifier, testing_set))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     24\u001b[0m LinearSVC_classifier \u001b[38;5;241m=\u001b[39m SklearnClassifier(LinearSVC())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\scikitlearn.py:112\u001b[0m, in \u001b[0;36mSklearnClassifier.train\u001b[1;34m(self, labeled_featuresets)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mTrain (fit) the scikit-learn estimator.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    numbers, booleans or strings.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mlabeled_featuresets))\n\u001b[1;32m--> 112\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:312\u001b[0m, in \u001b[0;36mDictVectorizer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03mLike fit(X) followed by transform(X), but does not require\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    Feature vectors; always 2-d.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:278\u001b[0m, in \u001b[0;36mDictVectorizer._transform\u001b[1;34m(self, X, fitting)\u001b[0m\n\u001b[0;32m    275\u001b[0m     result_matrix \u001b[38;5;241m=\u001b[39m result_matrix[:, map_index]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse:\n\u001b[1;32m--> 278\u001b[0m     \u001b[43mresult_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     result_matrix \u001b[38;5;241m=\u001b[39m result_matrix\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1164\u001b[0m, in \u001b[0;36m_cs_matrix.sort_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;124;03m\"\"\"Sort the indices of this matrix *in place*\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_sorted_indices:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[43m_sparsetools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsr_sort_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c300b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original Naive Bayes Algo accuracy percent: 63.0\n",
    "Most Informative Features\n",
    "                thematic = True              pos : neg    =      9.1 : 1.0\n",
    "                secondly = True              pos : neg    =      8.5 : 1.0\n",
    "                narrates = True              pos : neg    =      7.8 : 1.0\n",
    "                 rounded = True              pos : neg    =      7.1 : 1.0\n",
    "                 supreme = True              pos : neg    =      7.1 : 1.0\n",
    "                 layered = True              pos : neg    =      7.1 : 1.0\n",
    "                  crappy = True              neg : pos    =      6.9 : 1.0\n",
    "               uplifting = True              pos : neg    =      6.2 : 1.0\n",
    "                     ugh = True              neg : pos    =      5.3 : 1.0\n",
    "                   mamet = True              pos : neg    =      5.1 : 1.0\n",
    "                 gaining = True              pos : neg    =      5.1 : 1.0\n",
    "                   wanda = True              neg : pos    =      4.9 : 1.0\n",
    "                   onset = True              neg : pos    =      4.9 : 1.0\n",
    "               fantastic = True              pos : neg    =      4.5 : 1.0\n",
    "                kentucky = True              pos : neg    =      4.4 : 1.0\n",
    "MNB_classifier accuracy percent: 66.0\n",
    "BernoulliNB_classifier accuracy percent: 72.0\n",
    "LogisticRegression_classifier accuracy percent: 64.0\n",
    "SGDClassifier_classifier accuracy percent: 61.0\n",
    "SVC_classifier accuracy percent: 45.0\n",
    "LinearSVC_classifier accuracy percent: 68.0\n",
    "NuSVC_classifier accuracy percent: 59.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc33d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9da9338",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassifierI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVoteClassifier\u001b[39;00m(\u001b[43mClassifierI\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mclassifiers):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifiers \u001b[38;5;241m=\u001b[39m classifiers\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ClassifierI' is not defined"
     ]
    }
   ],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "999c1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ff42129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bccc0d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#classifier = nltk.NaiveBayesClassifier.train(training_set)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m classifier_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaivebayes.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m classifier_f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Naive Bayes Algo accuracy percent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, (nltk\u001b[38;5;241m.\u001b[39mclassify\u001b[38;5;241m.\u001b[39maccuracy(classifier, testing_set))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "        \n",
    "training_set = featuresets[:1900]\n",
    "testing_set =  featuresets[1900:]\n",
    "\n",
    "#classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\",\"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "##SVC_classifier = SklearnClassifier(SVC())\n",
    "##SVC_classifier.train(training_set)\n",
    "##print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8b70d40",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:10\u001b[1;36m\u001b[0m\n\u001b[1;33m    uplifting = True              pos : neg    =      6.2 : 1.0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Original Naive Bayes Algo accuracy percent: 66.0\n",
    "Most Informative Features\n",
    "                thematic = True              pos : neg    =      9.1 : 1.0\n",
    "                secondly = True              pos : neg    =      8.5 : 1.0\n",
    "                narrates = True              pos : neg    =      7.8 : 1.0\n",
    "                 layered = True              pos : neg    =      7.1 : 1.0\n",
    "                 rounded = True              pos : neg    =      7.1 : 1.0\n",
    "                 supreme = True              pos : neg    =      7.1 : 1.0\n",
    "                  crappy = True              neg : pos    =      6.9 : 1.0\n",
    "               uplifting = True              pos : neg    =      6.2 : 1.0\n",
    "                     ugh = True              neg : pos    =      5.3 : 1.0\n",
    "                 gaining = True              pos : neg    =      5.1 : 1.0\n",
    "                   mamet = True              pos : neg    =      5.1 : 1.0\n",
    "                   wanda = True              neg : pos    =      4.9 : 1.0\n",
    "                   onset = True              neg : pos    =      4.9 : 1.0\n",
    "               fantastic = True              pos : neg    =      4.5 : 1.0\n",
    "                   milos = True              pos : neg    =      4.4 : 1.0\n",
    "MNB_classifier accuracy percent: 67.0\n",
    "BernoulliNB_classifier accuracy percent: 67.0\n",
    "LogisticRegression_classifier accuracy percent: 68.0\n",
    "SGDClassifier_classifier accuracy percent: 57.99999999999999\n",
    "LinearSVC_classifier accuracy percent: 67.0\n",
    "NuSVC_classifier accuracy percent: 65.0\n",
    "voted_classifier accuracy percent: 65.0\n",
    "Classification: neg Confidence %: 100.0\n",
    "Classification: pos Confidence %: 57.14285714285714\n",
    "Classification: neg Confidence %: 57.14285714285714\n",
    "Classification: neg Confidence %: 57.14285714285714\n",
    "Classification: pos Confidence %: 57.14285714285714\n",
    "Classification: pos Confidence %: 85.71428571428571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "177b4b3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 25) (2833764312.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[60], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    Next, we also need to adjust our feature finding function, mainly tokenizing by word in the document, since we didn't have a nifty .words() feature for our new sample. I also went ahead and increased the most common words:\u001b[0m\n\u001b[1;37m                                                                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 25)\n"
     ]
    }
   ],
   "source": [
    "short_pos = open(\"short_reviews/positive.txt\",\"r\").read()\n",
    "short_neg = open(\"short_reviews/negative.txt\",\"r\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, \"neg\") )\n",
    "\n",
    "\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "Next, we also need to adjust our feature finding function, mainly tokenizing by word in the document, since we didn't have a nifty .words() feature for our new sample. I also went ahead and increased the most common words:\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\t\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "163a23f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'short_reviews/positive.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m         conf \u001b[38;5;241m=\u001b[39m choice_votes \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(votes)\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m conf\n\u001b[1;32m---> 38\u001b[0m short_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshort_reviews/positive.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     39\u001b[0m short_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_reviews/negative.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     41\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'short_reviews/positive.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "        \n",
    "short_pos = open(\"short_reviews/positive.txt\",\"r\").read()\n",
    "short_neg = open(\"short_reviews/negative.txt\",\"r\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, \"neg\") )\n",
    "\n",
    "\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# positive data example:      \n",
    "training_set = featuresets[:10000]\n",
    "testing_set =  featuresets[10000:]\n",
    "\n",
    "##\n",
    "### negative data example:      \n",
    "##training_set = featuresets[100:]\n",
    "##testing_set =  featuresets[:100]\n",
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "##SVC_classifier = SklearnClassifier(SVC())\n",
    "##SVC_classifier.train(training_set)\n",
    "##print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b27e34e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:9\u001b[1;36m\u001b[0m\n\u001b[1;33m    low-key = True              pos : neg    =      8.3 : 1.0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Original Naive Bayes Algo accuracy percent: 66.26506024096386\n",
    "Most Informative Features\n",
    "              refreshing = True              pos : neg    =     13.6 : 1.0\n",
    "                captures = True              pos : neg    =     11.3 : 1.0\n",
    "                  stupid = True              neg : pos    =     10.7 : 1.0\n",
    "                  tender = True              pos : neg    =      9.6 : 1.0\n",
    "              meandering = True              neg : pos    =      9.1 : 1.0\n",
    "                      tv = True              neg : pos    =      8.6 : 1.0\n",
    "                 low-key = True              pos : neg    =      8.3 : 1.0\n",
    "              thoughtful = True              pos : neg    =      8.1 : 1.0\n",
    "                   banal = True              neg : pos    =      7.7 : 1.0\n",
    "              amateurish = True              neg : pos    =      7.7 : 1.0\n",
    "                terrific = True              pos : neg    =      7.6 : 1.0\n",
    "                  record = True              pos : neg    =      7.6 : 1.0\n",
    "             captivating = True              pos : neg    =      7.6 : 1.0\n",
    "                portrait = True              pos : neg    =      7.4 : 1.0\n",
    "                 culture = True              pos : neg    =      7.3 : 1.0\n",
    "MNB_classifier accuracy percent: 65.8132530120482\n",
    "BernoulliNB_classifier accuracy percent: 66.71686746987952\n",
    "LogisticRegression_classifier accuracy percent: 67.16867469879519\n",
    "SGDClassifier_classifier accuracy percent: 65.8132530120482\n",
    "LinearSVC_classifier accuracy percent: 66.71686746987952\n",
    "NuSVC_classifier accuracy percent: 60.09036144578314\n",
    "voted_classifier accuracy percent: 65.66265060240963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02089de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'short_reviews/positive.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m         conf \u001b[38;5;241m=\u001b[39m choice_votes \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(votes)\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m conf\n\u001b[1;32m---> 36\u001b[0m short_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshort_reviews/positive.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     37\u001b[0m short_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_reviews/negative.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# move this up here\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'short_reviews/positive.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "    \n",
    "short_pos = open(\"short_reviews/positive.txt\",\"r\").read()\n",
    "short_neg = open(\"short_reviews/negative.txt\",\"r\").read()\n",
    "\n",
    "# move this up here\n",
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for p in short_pos.split('\\n'):\n",
    "    documents.append( (p, \"pos\") )\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "    \n",
    "for p in short_neg.split('\\n'):\n",
    "    documents.append( (p, \"neg\") )\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "\n",
    "\n",
    "save_documents = open(\"pickled_algos/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "\n",
    "save_word_features = open(\"pickled_algos/word_features5k.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "\n",
    "testing_set = featuresets[10000:]\n",
    "training_set = featuresets[:10000]\n",
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "###############\n",
    "save_classifier = open(\"pickled_algos/originalnaivebayes5k.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/MNB_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/LogisticRegression_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/LinearSVC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "\n",
    "##NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "##NuSVC_classifier.train(training_set)\n",
    "##print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/SGDC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(SGDC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0576083a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickled_algos/documents.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m         conf \u001b[38;5;241m=\u001b[39m choice_votes \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(votes)\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m conf\n\u001b[1;32m---> 39\u001b[0m documents_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickled_algos/documents.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m documents \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(documents_f)\n\u001b[0;32m     41\u001b[0m documents_f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickled_algos/documents.pickle'"
     ]
    }
   ],
   "source": [
    "#File: sentiment_mod.py\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "\n",
    "documents_f = open(\"pickled_algos/documents.pickle\", \"rb\")\n",
    "documents = pickle.load(documents_f)\n",
    "documents_f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_features5k_f = open(\"pickled_algos/word_features5k.pickle\", \"rb\")\n",
    "word_features = pickle.load(word_features5k_f)\n",
    "word_features5k_f.close()\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "featuresets_f = open(\"pickled_algos/featuresets.pickle\", \"rb\")\n",
    "featuresets = pickle.load(featuresets_f)\n",
    "featuresets_f.close()\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "\n",
    "testing_set = featuresets[10000:]\n",
    "training_set = featuresets[:10000]\n",
    "\n",
    "\n",
    "\n",
    "open_file = open(\"pickled_algos/originalnaivebayes5k.pickle\", \"rb\")\n",
    "classifier = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "open_file = open(\"pickled_algos/MNB_classifier5k.pickle\", \"rb\")\n",
    "MNB_classifier = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "\n",
    "open_file = open(\"pickled_algos/BernoulliNB_classifier5k.pickle\", \"rb\")\n",
    "BernoulliNB_classifier = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "open_file = open(\"pickled_algos/LogisticRegression_classifier5k.pickle\", \"rb\")\n",
    "LogisticRegression_classifier = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "open_file = open(\"pickled_algos/LinearSVC_classifier5k.pickle\", \"rb\")\n",
    "LinearSVC_classifier = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "open_file = open(\"pickled_algos/SGDC_classifier5k.pickle\", \"rb\")\n",
    "SGDC_classifier = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "                                  classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    return voted_classifier.classify(feats),voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcb77f9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentiment_mod'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentiment_mod\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01ms\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(s\u001b[38;5;241m.\u001b[39msentiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(s\u001b[38;5;241m.\u001b[39msentiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis movie was utter junk. There were absolutely 0 pythons. I don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt see what the point was at all. Horrible movie, 0/10\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentiment_mod'"
     ]
    }
   ],
   "source": [
    "import sentiment_mod as s\n",
    "\n",
    "print(s.sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
    "print(s.sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37701fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
